<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>notdiamond.toolkit.litellm.main &mdash; NotDiamond 0.3.36</title>
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../../../_static/documentation_options.js?v=ec281a4a"></script>
        <script src="../../../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            NotDiamond
          </a>
              <div class="version">
                0.3.36
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/intro.html">Getting started with Not Diamond</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/notdiamond.html">NotDiamond package</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">NotDiamond</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">notdiamond.toolkit.litellm.main</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for notdiamond.toolkit.litellm.main</h1><div class="highlight"><pre>
<span></span><span class="c1"># flake8: noqa</span>

<span class="c1"># This file is a modified version of the original module provided by BerriAI.</span>
<span class="c1"># We have modified the file to add support for Not Diamond, and include the following</span>
<span class="c1"># license to comply with their license requirements:</span>

<span class="c1"># MIT License</span>

<span class="c1"># Copyright (c) 2023 Berri AI</span>

<span class="c1"># Permission is hereby granted, free of charge, to any person obtaining a copy</span>
<span class="c1"># of this software and associated documentation files (the &quot;Software&quot;), to deal</span>
<span class="c1"># in the Software without restriction, including without limitation the rights</span>
<span class="c1"># to use, copy, modify, merge, publish, distribute, sublicense, and/or sell</span>
<span class="c1"># copies of the Software, and to permit persons to whom the Software is</span>
<span class="c1"># furnished to do so, subject to the following conditions:</span>

<span class="c1"># The above copyright notice and this permission notice shall be included in all</span>
<span class="c1"># copies or substantial portions of the Software.</span>

<span class="c1"># THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR</span>
<span class="c1"># IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,</span>
<span class="c1"># FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE</span>
<span class="c1"># AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER</span>
<span class="c1"># LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,</span>
<span class="c1"># OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE</span>
<span class="c1"># SOFTWARE.</span>


<span class="kn">import</span> <span class="nn">asyncio</span>
<span class="kn">import</span> <span class="nn">contextvars</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Type</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">httpx</span>
<span class="kn">import</span> <span class="nn">litellm</span>
<span class="kn">import</span> <span class="nn">openai</span>
<span class="kn">import</span> <span class="nn">tiktoken</span>
<span class="kn">from</span> <span class="nn">litellm</span> <span class="kn">import</span> <span class="p">(</span>  <span class="c1"># type: ignore</span>
    <span class="n">client</span><span class="p">,</span>
    <span class="n">exception_type</span><span class="p">,</span>
    <span class="n">get_litellm_params</span><span class="p">,</span>
    <span class="n">get_optional_params</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">litellm._logging</span> <span class="kn">import</span> <span class="n">verbose_logger</span>
<span class="kn">from</span> <span class="nn">litellm.llms</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">aleph_alpha</span><span class="p">,</span>
    <span class="n">baseten</span><span class="p">,</span>
    <span class="n">clarifai</span><span class="p">,</span>
    <span class="n">cloudflare</span><span class="p">,</span>
    <span class="n">maritalk</span><span class="p">,</span>
    <span class="n">nlp_cloud</span><span class="p">,</span>
    <span class="n">ollama</span><span class="p">,</span>
    <span class="n">ollama_chat</span><span class="p">,</span>
    <span class="n">oobabooga</span><span class="p">,</span>
    <span class="n">openrouter</span><span class="p">,</span>
    <span class="n">palm</span><span class="p">,</span>
    <span class="n">petals</span><span class="p">,</span>
    <span class="n">replicate</span><span class="p">,</span>
    <span class="n">vllm</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">litellm.llms.AI21</span> <span class="kn">import</span> <span class="n">completion</span> <span class="k">as</span> <span class="n">ai21</span>
<span class="kn">from</span> <span class="nn">litellm.llms.AzureOpenAI.azure</span> <span class="kn">import</span> <span class="n">_check_dynamic_azure_params</span>
<span class="kn">from</span> <span class="nn">litellm.llms.cohere</span> <span class="kn">import</span> <span class="n">chat</span> <span class="k">as</span> <span class="n">cohere_chat</span>
<span class="kn">from</span> <span class="nn">litellm.llms.cohere</span> <span class="kn">import</span> <span class="n">completion</span> <span class="k">as</span> <span class="n">cohere_completion</span>  <span class="c1"># type: ignore</span>
<span class="kn">from</span> <span class="nn">litellm.llms.custom_llm</span> <span class="kn">import</span> <span class="n">CustomLLM</span><span class="p">,</span> <span class="n">custom_chat_llm_router</span>
<span class="kn">from</span> <span class="nn">litellm.llms.prompt_templates.factory</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">custom_prompt</span><span class="p">,</span>
    <span class="n">function_call_prompt</span><span class="p">,</span>
    <span class="n">map_system_message_pt</span><span class="p">,</span>
    <span class="n">prompt_factory</span><span class="p">,</span>
    <span class="n">stringify_json_tool_call_content</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">litellm.llms.vertex_ai_and_google_ai_studio</span> <span class="kn">import</span> <span class="n">vertex_ai_non_gemini</span>
<span class="kn">from</span> <span class="nn">litellm.main</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">litellm.types.router</span> <span class="kn">import</span> <span class="n">LiteLLM_Params</span>
<span class="kn">from</span> <span class="nn">litellm.types.utils</span> <span class="kn">import</span> <span class="n">all_litellm_params</span>
<span class="kn">from</span> <span class="nn">litellm.utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">CustomStreamWrapper</span><span class="p">,</span>
    <span class="n">ModelResponse</span><span class="p">,</span>
    <span class="n">TextCompletionResponse</span><span class="p">,</span>
    <span class="n">completion_with_fallbacks</span><span class="p">,</span>
    <span class="n">get_secret</span><span class="p">,</span>
    <span class="n">supports_httpx_timeout</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span>

<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">notdiamond_key</span><span class="p">,</span> <span class="n">provider_list</span>
<span class="kn">from</span> <span class="nn">.litellm_notdiamond</span> <span class="kn">import</span> <span class="n">completion</span> <span class="k">as</span> <span class="n">notdiamond_completion</span>

<span class="n">encoding</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">get_encoding</span><span class="p">(</span><span class="s2">&quot;cl100k_base&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="get_api_key">
<a class="viewcode-back" href="../../../../source/notdiamond.toolkit.litellm.html#notdiamond.toolkit.litellm.main.get_api_key">[docs]</a>
<span class="k">def</span> <span class="nf">get_api_key</span><span class="p">(</span><span class="n">llm_provider</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">dynamic_api_key</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]):</span>
    <span class="n">api_key</span> <span class="o">=</span> <span class="n">dynamic_api_key</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_key</span>
    <span class="c1"># openai</span>
    <span class="k">if</span> <span class="n">llm_provider</span> <span class="o">==</span> <span class="s2">&quot;openai&quot;</span> <span class="ow">or</span> <span class="n">llm_provider</span> <span class="o">==</span> <span class="s2">&quot;text-completion-openai&quot;</span><span class="p">:</span>
        <span class="n">api_key</span> <span class="o">=</span> <span class="n">api_key</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">openai_key</span> <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">)</span>
    <span class="c1"># anthropic</span>
    <span class="k">elif</span> <span class="n">llm_provider</span> <span class="o">==</span> <span class="s2">&quot;anthropic&quot;</span><span class="p">:</span>
        <span class="n">api_key</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">api_key</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">anthropic_key</span> <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;ANTHROPIC_API_KEY&quot;</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="c1"># ai21</span>
    <span class="k">elif</span> <span class="n">llm_provider</span> <span class="o">==</span> <span class="s2">&quot;ai21&quot;</span><span class="p">:</span>
        <span class="n">api_key</span> <span class="o">=</span> <span class="n">api_key</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">ai21_key</span> <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;AI211_API_KEY&quot;</span><span class="p">)</span>
    <span class="c1"># aleph_alpha</span>
    <span class="k">elif</span> <span class="n">llm_provider</span> <span class="o">==</span> <span class="s2">&quot;aleph_alpha&quot;</span><span class="p">:</span>
        <span class="n">api_key</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">api_key</span>
            <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">aleph_alpha_key</span>
            <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;ALEPH_ALPHA_API_KEY&quot;</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="c1"># baseten</span>
    <span class="k">elif</span> <span class="n">llm_provider</span> <span class="o">==</span> <span class="s2">&quot;baseten&quot;</span><span class="p">:</span>
        <span class="n">api_key</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">api_key</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">baseten_key</span> <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;BASETEN_API_KEY&quot;</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="c1"># cohere</span>
    <span class="k">elif</span> <span class="n">llm_provider</span> <span class="o">==</span> <span class="s2">&quot;cohere&quot;</span> <span class="ow">or</span> <span class="n">llm_provider</span> <span class="o">==</span> <span class="s2">&quot;cohere_chat&quot;</span><span class="p">:</span>
        <span class="n">api_key</span> <span class="o">=</span> <span class="n">api_key</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">cohere_key</span> <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;COHERE_API_KEY&quot;</span><span class="p">)</span>
    <span class="c1"># huggingface</span>
    <span class="k">elif</span> <span class="n">llm_provider</span> <span class="o">==</span> <span class="s2">&quot;huggingface&quot;</span><span class="p">:</span>
        <span class="n">api_key</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">api_key</span>
            <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">huggingface_key</span>
            <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;HUGGINGFACE_API_KEY&quot;</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="c1"># notdiamond</span>
    <span class="k">elif</span> <span class="n">llm_provider</span> <span class="o">==</span> <span class="s2">&quot;notdiamond&quot;</span><span class="p">:</span>
        <span class="n">api_key</span> <span class="o">=</span> <span class="n">api_key</span> <span class="ow">or</span> <span class="n">notdiamond_key</span> <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;NOTDIAMOND_API_KEY&quot;</span><span class="p">)</span>
    <span class="c1"># nlp_cloud</span>
    <span class="k">elif</span> <span class="n">llm_provider</span> <span class="o">==</span> <span class="s2">&quot;nlp_cloud&quot;</span><span class="p">:</span>
        <span class="n">api_key</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">api_key</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">nlp_cloud_key</span> <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;NLP_CLOUD_API_KEY&quot;</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="c1"># replicate</span>
    <span class="k">elif</span> <span class="n">llm_provider</span> <span class="o">==</span> <span class="s2">&quot;replicate&quot;</span><span class="p">:</span>
        <span class="n">api_key</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">api_key</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">replicate_key</span> <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;REPLICATE_API_KEY&quot;</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="c1"># together_ai</span>
    <span class="k">elif</span> <span class="n">llm_provider</span> <span class="o">==</span> <span class="s2">&quot;together_ai&quot;</span><span class="p">:</span>
        <span class="n">api_key</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">api_key</span>
            <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">togetherai_api_key</span>
            <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;TOGETHERAI_API_KEY&quot;</span><span class="p">)</span>
            <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;TOGETHER_AI_TOKEN&quot;</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">api_key</span></div>



<div class="viewcode-block" id="get_llm_provider">
<a class="viewcode-back" href="../../../../source/notdiamond.toolkit.litellm.html#notdiamond.toolkit.litellm.main.get_llm_provider">[docs]</a>
<span class="k">def</span> <span class="nf">get_llm_provider</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">custom_llm_provider</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">api_base</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">api_key</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">litellm_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LiteLLM_Params</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the provider for a given model name - e.g. &#39;azure/chatgpt-v-2&#39; -&gt; &#39;azure&#39;</span>

<span class="sd">    For router -&gt; Can also give the whole litellm param dict -&gt; this function will extract the relevant details</span>

<span class="sd">    Raises Error - if unable to map model to a provider</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="c1">## IF LITELLM PARAMS GIVEN ##</span>
        <span class="k">if</span> <span class="n">litellm_params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">custom_llm_provider</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="n">api_base</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="n">api_key</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="p">),</span> <span class="s2">&quot;Either pass in litellm_params or the custom_llm_provider/api_base/api_key. Otherwise, these values will be overriden.&quot;</span>
            <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="n">litellm_params</span><span class="o">.</span><span class="n">custom_llm_provider</span>
            <span class="n">api_base</span> <span class="o">=</span> <span class="n">litellm_params</span><span class="o">.</span><span class="n">api_base</span>
            <span class="n">api_key</span> <span class="o">=</span> <span class="n">litellm_params</span><span class="o">.</span><span class="n">api_key</span>

        <span class="n">dynamic_api_key</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># check if llm provider provided</span>
        <span class="c1"># AZURE AI-Studio Logic - Azure AI Studio supports AZURE/Cohere</span>
        <span class="c1"># If User passes azure/command-r-plus -&gt; we should send it to cohere_chat/command-r-plus</span>
        <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;azure&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">_is_non_openai_azure_model</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
                <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;openai&quot;</span>
                <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">custom_llm_provider</span><span class="p">,</span> <span class="n">dynamic_api_key</span><span class="p">,</span> <span class="n">api_base</span>

        <span class="k">if</span> <span class="n">custom_llm_provider</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">custom_llm_provider</span><span class="p">,</span> <span class="n">dynamic_api_key</span><span class="p">,</span> <span class="n">api_base</span>

        <span class="k">if</span> <span class="n">api_key</span> <span class="ow">and</span> <span class="n">api_key</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;os.environ/&quot;</span><span class="p">):</span>
            <span class="n">dynamic_api_key</span> <span class="o">=</span> <span class="n">get_secret</span><span class="p">(</span><span class="n">api_key</span><span class="p">)</span>
        <span class="c1"># check if llm provider part of model name</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">model</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">in</span> <span class="n">provider_list</span>
            <span class="ow">and</span> <span class="n">model</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">model_list</span>
            <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">))</span>
            <span class="o">&gt;</span> <span class="mi">1</span>  <span class="c1"># handle edge case where user passes in `litellm --model mistral` https://github.com/BerriAI/litellm/issues/1351</span>
        <span class="p">):</span>
            <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;perplexity&quot;</span><span class="p">:</span>
                <span class="c1"># perplexity is openai compatible, we just need to set this to custom_openai and have the api_base be https://api.perplexity.ai</span>
                <span class="n">api_base</span> <span class="o">=</span> <span class="n">api_base</span> <span class="ow">or</span> <span class="s2">&quot;https://api.perplexity.ai&quot;</span>
                <span class="n">dynamic_api_key</span> <span class="o">=</span> <span class="n">api_key</span> <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;PERPLEXITYAI_API_KEY&quot;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;anyscale&quot;</span><span class="p">:</span>
                <span class="c1"># anyscale is openai compatible, we just need to set this to custom_openai and have the api_base be https://api.endpoints.anyscale.com/v1</span>
                <span class="n">api_base</span> <span class="o">=</span> <span class="n">api_base</span> <span class="ow">or</span> <span class="s2">&quot;https://api.endpoints.anyscale.com/v1&quot;</span>
                <span class="n">dynamic_api_key</span> <span class="o">=</span> <span class="n">api_key</span> <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;ANYSCALE_API_KEY&quot;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;deepinfra&quot;</span><span class="p">:</span>
                <span class="c1"># deepinfra is openai compatible, we just need to set this to custom_openai and have the api_base be https://api.endpoints.anyscale.com/v1</span>
                <span class="n">api_base</span> <span class="o">=</span> <span class="n">api_base</span> <span class="ow">or</span> <span class="s2">&quot;https://api.deepinfra.com/v1/openai&quot;</span>
                <span class="n">dynamic_api_key</span> <span class="o">=</span> <span class="n">api_key</span> <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;DEEPINFRA_API_KEY&quot;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;empower&quot;</span><span class="p">:</span>
                <span class="n">api_base</span> <span class="o">=</span> <span class="n">api_base</span> <span class="ow">or</span> <span class="s2">&quot;https://app.empower.dev/api/v1&quot;</span>
                <span class="n">dynamic_api_key</span> <span class="o">=</span> <span class="n">api_key</span> <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;EMPOWER_API_KEY&quot;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;groq&quot;</span><span class="p">:</span>
                <span class="c1"># groq is openai compatible, we just need to set this to custom_openai and have the api_base be https://api.groq.com/openai/v1</span>
                <span class="n">api_base</span> <span class="o">=</span> <span class="n">api_base</span> <span class="ow">or</span> <span class="s2">&quot;https://api.groq.com/openai/v1&quot;</span>
                <span class="n">dynamic_api_key</span> <span class="o">=</span> <span class="n">api_key</span> <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;GROQ_API_KEY&quot;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;nvidia_nim&quot;</span><span class="p">:</span>
                <span class="c1"># nvidia_nim is openai compatible, we just need to set this to custom_openai and have the api_base be https://api.endpoints.anyscale.com/v1</span>
                <span class="n">api_base</span> <span class="o">=</span> <span class="n">api_base</span> <span class="ow">or</span> <span class="s2">&quot;https://integrate.api.nvidia.com/v1&quot;</span>
                <span class="n">dynamic_api_key</span> <span class="o">=</span> <span class="n">api_key</span> <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;NVIDIA_NIM_API_KEY&quot;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;volcengine&quot;</span><span class="p">:</span>
                <span class="c1"># volcengine is openai compatible, we just need to set this to custom_openai and have the api_base be https://api.endpoints.anyscale.com/v1</span>
                <span class="n">api_base</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">api_base</span> <span class="ow">or</span> <span class="s2">&quot;https://ark.cn-beijing.volces.com/api/v3&quot;</span>
                <span class="p">)</span>
                <span class="n">dynamic_api_key</span> <span class="o">=</span> <span class="n">api_key</span> <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;VOLCENGINE_API_KEY&quot;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;codestral&quot;</span><span class="p">:</span>
                <span class="c1"># codestral is openai compatible, we just need to set this to custom_openai and have the api_base be https://codestral.mistral.ai/v1</span>
                <span class="n">api_base</span> <span class="o">=</span> <span class="n">api_base</span> <span class="ow">or</span> <span class="s2">&quot;https://codestral.mistral.ai/v1&quot;</span>
                <span class="n">dynamic_api_key</span> <span class="o">=</span> <span class="n">api_key</span> <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;CODESTRAL_API_KEY&quot;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;deepseek&quot;</span><span class="p">:</span>
                <span class="c1"># deepseek is openai compatible, we just need to set this to custom_openai and have the api_base be https://api.deepseek.com/v1</span>
                <span class="n">api_base</span> <span class="o">=</span> <span class="n">api_base</span> <span class="ow">or</span> <span class="s2">&quot;https://api.deepseek.com/v1&quot;</span>
                <span class="n">dynamic_api_key</span> <span class="o">=</span> <span class="n">api_key</span> <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;DEEPSEEK_API_KEY&quot;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;fireworks_ai&quot;</span><span class="p">:</span>
                <span class="c1"># fireworks is openai compatible, we just need to set this to custom_openai and have the api_base be https://api.fireworks.ai/inference/v1</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">model</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;accounts/&quot;</span><span class="p">):</span>
                    <span class="n">model</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;accounts/fireworks/models/</span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="n">api_base</span> <span class="o">=</span> <span class="n">api_base</span> <span class="ow">or</span> <span class="s2">&quot;https://api.fireworks.ai/inference/v1&quot;</span>
                <span class="n">dynamic_api_key</span> <span class="o">=</span> <span class="n">api_key</span> <span class="ow">or</span> <span class="p">(</span>
                    <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;FIREWORKS_API_KEY&quot;</span><span class="p">)</span>
                    <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;FIREWORKS_AI_API_KEY&quot;</span><span class="p">)</span>
                    <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;FIREWORKSAI_API_KEY&quot;</span><span class="p">)</span>
                    <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;FIREWORKS_AI_TOKEN&quot;</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;azure_ai&quot;</span><span class="p">:</span>
                <span class="n">api_base</span> <span class="o">=</span> <span class="n">api_base</span> <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;AZURE_AI_API_BASE&quot;</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
                <span class="n">dynamic_api_key</span> <span class="o">=</span> <span class="n">api_key</span> <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;AZURE_AI_API_KEY&quot;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;mistral&quot;</span><span class="p">:</span>
                <span class="c1"># mistral is openai compatible, we just need to set this to custom_openai and have the api_base be https://api.mistral.ai</span>
                <span class="n">api_base</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">api_base</span>
                    <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span>
                        <span class="s2">&quot;MISTRAL_AZURE_API_BASE&quot;</span>
                    <span class="p">)</span>  <span class="c1"># for Azure AI Mistral</span>
                    <span class="ow">or</span> <span class="s2">&quot;https://api.mistral.ai/v1&quot;</span>
                <span class="p">)</span>  <span class="c1"># type: ignore</span>

                <span class="c1"># if api_base does not end with /v1 we add it</span>
                <span class="k">if</span> <span class="n">api_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">api_base</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span>
                    <span class="s2">&quot;/v1&quot;</span>
                <span class="p">):</span>  <span class="c1"># Mistral always needs a /v1 at the end</span>
                    <span class="n">api_base</span> <span class="o">=</span> <span class="n">api_base</span> <span class="o">+</span> <span class="s2">&quot;/v1&quot;</span>
                <span class="n">dynamic_api_key</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">api_key</span>
                    <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span>
                        <span class="s2">&quot;MISTRAL_AZURE_API_KEY&quot;</span>
                    <span class="p">)</span>  <span class="c1"># for Azure AI Mistral</span>
                    <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;MISTRAL_API_KEY&quot;</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;voyage&quot;</span><span class="p">:</span>
                <span class="c1"># voyage is openai compatible, we just need to set this to custom_openai and have the api_base be https://api.voyageai.com/v1</span>
                <span class="n">api_base</span> <span class="o">=</span> <span class="s2">&quot;https://api.voyageai.com/v1&quot;</span>
                <span class="n">dynamic_api_key</span> <span class="o">=</span> <span class="n">api_key</span> <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;VOYAGE_API_KEY&quot;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;together_ai&quot;</span><span class="p">:</span>
                <span class="n">api_base</span> <span class="o">=</span> <span class="s2">&quot;https://api.together.xyz/v1&quot;</span>
                <span class="n">dynamic_api_key</span> <span class="o">=</span> <span class="n">api_key</span> <span class="ow">or</span> <span class="p">(</span>
                    <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;TOGETHER_API_KEY&quot;</span><span class="p">)</span>
                    <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;TOGETHER_AI_API_KEY&quot;</span><span class="p">)</span>
                    <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;TOGETHERAI_API_KEY&quot;</span><span class="p">)</span>
                    <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;TOGETHER_AI_TOKEN&quot;</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;friendliai&quot;</span><span class="p">:</span>
                <span class="n">api_base</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">api_base</span>
                    <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;FRIENDLI_API_BASE&quot;</span><span class="p">)</span>
                    <span class="ow">or</span> <span class="s2">&quot;https://inference.friendli.ai/v1&quot;</span>
                <span class="p">)</span>
                <span class="n">dynamic_api_key</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">api_key</span>
                    <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;FRIENDLIAI_API_KEY&quot;</span><span class="p">)</span>
                    <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;FRIENDLI_TOKEN&quot;</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;notdiamond&quot;</span><span class="p">:</span>
                <span class="n">api_base</span> <span class="o">=</span> <span class="s2">&quot;/&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">get_secret</span><span class="p">(</span>
                            <span class="s2">&quot;NOTDIAMOND_API_URL&quot;</span><span class="p">,</span> <span class="s2">&quot;https://api.notdiamond.ai&quot;</span>
                        <span class="p">),</span>
                        <span class="s2">&quot;v2/optimizer/modelSelect&quot;</span><span class="p">,</span>
                    <span class="p">]</span>
                <span class="p">)</span>
                <span class="n">dynamic_api_key</span> <span class="o">=</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;NOTDIAMOND_API_KEY&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="n">api_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">api_base</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
                    <span class="s2">&quot;api base needs to be a string. api_base=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">api_base</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">dynamic_api_key</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span>
                <span class="n">dynamic_api_key</span><span class="p">,</span> <span class="nb">str</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
                    <span class="s2">&quot;dynamic_api_key needs to be a string. dynamic_api_key=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">dynamic_api_key</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">custom_llm_provider</span><span class="p">,</span> <span class="n">dynamic_api_key</span><span class="p">,</span> <span class="n">api_base</span>
        <span class="k">elif</span> <span class="n">model</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">in</span> <span class="n">provider_list</span><span class="p">:</span>
            <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">api_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">api_base</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
                    <span class="s2">&quot;api base needs to be a string. api_base=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">api_base</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">dynamic_api_key</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span>
                <span class="n">dynamic_api_key</span><span class="p">,</span> <span class="nb">str</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
                    <span class="s2">&quot;dynamic_api_key needs to be a string. dynamic_api_key=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">dynamic_api_key</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">custom_llm_provider</span><span class="p">,</span> <span class="n">dynamic_api_key</span><span class="p">,</span> <span class="n">api_base</span>
        <span class="c1"># check if api base is a known openai compatible endpoint</span>
        <span class="k">if</span> <span class="n">api_base</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">endpoint</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">openai_compatible_endpoints</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">endpoint</span> <span class="ow">in</span> <span class="n">api_base</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">endpoint</span> <span class="o">==</span> <span class="s2">&quot;api.perplexity.ai&quot;</span><span class="p">:</span>
                        <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;perplexity&quot;</span>
                        <span class="n">dynamic_api_key</span> <span class="o">=</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;PERPLEXITYAI_API_KEY&quot;</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="n">endpoint</span> <span class="o">==</span> <span class="s2">&quot;api.endpoints.anyscale.com/v1&quot;</span><span class="p">:</span>
                        <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;anyscale&quot;</span>
                        <span class="n">dynamic_api_key</span> <span class="o">=</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;ANYSCALE_API_KEY&quot;</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="n">endpoint</span> <span class="o">==</span> <span class="s2">&quot;api.deepinfra.com/v1/openai&quot;</span><span class="p">:</span>
                        <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;deepinfra&quot;</span>
                        <span class="n">dynamic_api_key</span> <span class="o">=</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;DEEPINFRA_API_KEY&quot;</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="n">endpoint</span> <span class="o">==</span> <span class="s2">&quot;api.mistral.ai/v1&quot;</span><span class="p">:</span>
                        <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;mistral&quot;</span>
                        <span class="n">dynamic_api_key</span> <span class="o">=</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;MISTRAL_API_KEY&quot;</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="n">endpoint</span> <span class="o">==</span> <span class="s2">&quot;api.groq.com/openai/v1&quot;</span><span class="p">:</span>
                        <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;groq&quot;</span>
                        <span class="n">dynamic_api_key</span> <span class="o">=</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;GROQ_API_KEY&quot;</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="n">endpoint</span> <span class="o">==</span> <span class="s2">&quot;https://integrate.api.nvidia.com/v1&quot;</span><span class="p">:</span>
                        <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;nvidia_nim&quot;</span>
                        <span class="n">dynamic_api_key</span> <span class="o">=</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;NVIDIA_NIM_API_KEY&quot;</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="n">endpoint</span> <span class="o">==</span> <span class="s2">&quot;https://codestral.mistral.ai/v1&quot;</span><span class="p">:</span>
                        <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;codestral&quot;</span>
                        <span class="n">dynamic_api_key</span> <span class="o">=</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;CODESTRAL_API_KEY&quot;</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="n">endpoint</span> <span class="o">==</span> <span class="s2">&quot;https://codestral.mistral.ai/v1&quot;</span><span class="p">:</span>
                        <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;text-completion-codestral&quot;</span>
                        <span class="n">dynamic_api_key</span> <span class="o">=</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;CODESTRAL_API_KEY&quot;</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="n">endpoint</span> <span class="o">==</span> <span class="s2">&quot;app.empower.dev/api/v1&quot;</span><span class="p">:</span>
                        <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;empower&quot;</span>
                        <span class="n">dynamic_api_key</span> <span class="o">=</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;EMPOWER_API_KEY&quot;</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="n">endpoint</span> <span class="o">==</span> <span class="s2">&quot;api.deepseek.com/v1&quot;</span><span class="p">:</span>
                        <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;deepseek&quot;</span>
                        <span class="n">dynamic_api_key</span> <span class="o">=</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;DEEPSEEK_API_KEY&quot;</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="n">endpoint</span> <span class="o">==</span> <span class="s2">&quot;inference.friendli.ai/v1&quot;</span><span class="p">:</span>
                        <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;friendliai&quot;</span>
                        <span class="n">dynamic_api_key</span> <span class="o">=</span> <span class="n">get_secret</span><span class="p">(</span>
                            <span class="s2">&quot;FRIENDLIAI_API_KEY&quot;</span>
                        <span class="p">)</span> <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;FRIENDLI_TOKEN&quot;</span><span class="p">)</span>

                    <span class="k">if</span> <span class="n">api_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">api_base</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
                            <span class="s2">&quot;api base needs to be a string. api_base=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                                <span class="n">api_base</span>
                            <span class="p">)</span>
                        <span class="p">)</span>
                    <span class="k">if</span> <span class="n">dynamic_api_key</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span>
                        <span class="n">dynamic_api_key</span><span class="p">,</span> <span class="nb">str</span>
                    <span class="p">):</span>
                        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
                            <span class="s2">&quot;dynamic_api_key needs to be a string. dynamic_api_key=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                                <span class="n">dynamic_api_key</span>
                            <span class="p">)</span>
                        <span class="p">)</span>
                    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">custom_llm_provider</span><span class="p">,</span> <span class="n">dynamic_api_key</span><span class="p">,</span> <span class="n">api_base</span>  <span class="c1"># type: ignore</span>

        <span class="c1"># check if model in known model provider list  -&gt; for huggingface models, raise exception as they don&#39;t have a fixed provider (can be togetherai, anyscale, baseten, runpod, et.)</span>
        <span class="c1">## openai - chatcompletion + text completion</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">open_ai_chat_completion_models</span>
            <span class="ow">or</span> <span class="s2">&quot;ft:gpt-3.5-turbo&quot;</span> <span class="ow">in</span> <span class="n">model</span>
            <span class="ow">or</span> <span class="s2">&quot;ft:gpt-4&quot;</span> <span class="ow">in</span> <span class="n">model</span>  <span class="c1"># catches ft:gpt-4-0613, ft:gpt-4o</span>
            <span class="ow">or</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">openai_image_generation_models</span>
        <span class="p">):</span>
            <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;openai&quot;</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">open_ai_text_completion_models</span><span class="p">:</span>
            <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;text-completion-openai&quot;</span>
        <span class="c1">## anthropic</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">anthropic_models</span><span class="p">:</span>
            <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;anthropic&quot;</span>
        <span class="c1">## cohere</span>
        <span class="k">elif</span> <span class="p">(</span>
            <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">cohere_models</span>
            <span class="ow">or</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">cohere_embedding_models</span>
        <span class="p">):</span>
            <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;cohere&quot;</span>
        <span class="c1">## cohere chat models</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">cohere_chat_models</span><span class="p">:</span>
            <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;cohere_chat&quot;</span>
        <span class="c1">## replicate</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">replicate_models</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="s2">&quot;:&quot;</span> <span class="ow">in</span> <span class="n">model</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">64</span>
        <span class="p">):</span>
            <span class="n">model_parts</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;:&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">model_parts</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">model_parts</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="mi">64</span>
            <span class="p">):</span>  <span class="c1">## checks if model name has a 64 digit code - e.g. &quot;meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3&quot;</span>
                <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;replicate&quot;</span>
            <span class="k">elif</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">replicate_models</span><span class="p">:</span>
                <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;replicate&quot;</span>
        <span class="c1">## openrouter</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">openrouter_models</span><span class="p">:</span>
            <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;openrouter&quot;</span>
        <span class="c1">## openrouter</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">maritalk_models</span><span class="p">:</span>
            <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;maritalk&quot;</span>
        <span class="c1">## vertex - text + chat + language (gemini) models</span>
        <span class="k">elif</span> <span class="p">(</span>
            <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">vertex_chat_models</span>
            <span class="ow">or</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">vertex_code_chat_models</span>
            <span class="ow">or</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">vertex_text_models</span>
            <span class="ow">or</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">vertex_code_text_models</span>
            <span class="ow">or</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">vertex_language_models</span>
            <span class="ow">or</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">vertex_embedding_models</span>
            <span class="ow">or</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">vertex_vision_models</span>
        <span class="p">):</span>
            <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;vertex_ai&quot;</span>
        <span class="c1">## ai21</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">ai21_models</span><span class="p">:</span>
            <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;ai21&quot;</span>
        <span class="c1">## aleph_alpha</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">aleph_alpha_models</span><span class="p">:</span>
            <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;aleph_alpha&quot;</span>
        <span class="c1">## baseten</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">baseten_models</span><span class="p">:</span>
            <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;baseten&quot;</span>
        <span class="c1">## nlp_cloud</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">nlp_cloud_models</span><span class="p">:</span>
            <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;nlp_cloud&quot;</span>
        <span class="c1">## petals</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">petals_models</span><span class="p">:</span>
            <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;petals&quot;</span>
        <span class="c1">## bedrock</span>
        <span class="k">elif</span> <span class="p">(</span>
            <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">bedrock_models</span>
            <span class="ow">or</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">bedrock_embedding_models</span>
        <span class="p">):</span>
            <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;bedrock&quot;</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">watsonx_models</span><span class="p">:</span>
            <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;watsonx&quot;</span>
        <span class="c1"># openai embeddings</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">open_ai_embedding_models</span><span class="p">:</span>
            <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;openai&quot;</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">empower_models</span><span class="p">:</span>
            <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;empower&quot;</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;*&quot;</span><span class="p">:</span>
            <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;openai&quot;</span>
        <span class="k">if</span> <span class="n">custom_llm_provider</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">litellm</span><span class="o">.</span><span class="n">suppress_debug_info</span> <span class="o">==</span> <span class="kc">False</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">()</span>  <span class="c1"># noqa</span>
                <span class="nb">print</span><span class="p">(</span>  <span class="c1"># noqa</span>
                    <span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[1;31mProvider List: https://docs.litellm.ai/docs/providers</span><span class="se">\033</span><span class="s2">[0m&quot;</span>  <span class="c1"># noqa</span>
                <span class="p">)</span>  <span class="c1"># noqa</span>
                <span class="nb">print</span><span class="p">()</span>  <span class="c1"># noqa</span>
            <span class="n">error_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=</span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="se">\n</span><span class="s2"> Pass model as E.g. For &#39;Huggingface&#39; inference endpoints pass in `completion(model=&#39;huggingface/starcoder&#39;,..)` Learn more: https://docs.litellm.ai/docs/providers&quot;</span>
            <span class="c1"># maps to openai.NotFoundError, this is raised when openai does not recognize the llm</span>
            <span class="k">raise</span> <span class="n">litellm</span><span class="o">.</span><span class="n">exceptions</span><span class="o">.</span><span class="n">BadRequestError</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
                <span class="n">message</span><span class="o">=</span><span class="n">error_str</span><span class="p">,</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">response</span><span class="o">=</span><span class="n">httpx</span><span class="o">.</span><span class="n">Response</span><span class="p">(</span>
                    <span class="n">status_code</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span>
                    <span class="n">content</span><span class="o">=</span><span class="n">error_str</span><span class="p">,</span>
                    <span class="n">request</span><span class="o">=</span><span class="n">httpx</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;completion&quot;</span><span class="p">,</span> <span class="n">url</span><span class="o">=</span><span class="s2">&quot;https://github.com/BerriAI/litellm&quot;</span><span class="p">),</span>  <span class="c1"># type: ignore</span>
                <span class="p">),</span>
                <span class="n">llm_provider</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">api_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">api_base</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
                <span class="s2">&quot;api base needs to be a string. api_base=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">api_base</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">dynamic_api_key</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">dynamic_api_key</span><span class="p">,</span> <span class="nb">str</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
                <span class="s2">&quot;dynamic_api_key needs to be a string. dynamic_api_key=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">dynamic_api_key</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">custom_llm_provider</span><span class="p">,</span> <span class="n">dynamic_api_key</span><span class="p">,</span> <span class="n">api_base</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">litellm</span><span class="o">.</span><span class="n">exceptions</span><span class="o">.</span><span class="n">BadRequestError</span><span class="p">):</span>
            <span class="k">raise</span> <span class="n">e</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">error_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;GetLLMProvider Exception - </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">original model: </span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">raise</span> <span class="n">litellm</span><span class="o">.</span><span class="n">exceptions</span><span class="o">.</span><span class="n">BadRequestError</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
                <span class="n">message</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;GetLLMProvider Exception - </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">original model: </span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">response</span><span class="o">=</span><span class="n">httpx</span><span class="o">.</span><span class="n">Response</span><span class="p">(</span>
                    <span class="n">status_code</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span>
                    <span class="n">content</span><span class="o">=</span><span class="n">error_str</span><span class="p">,</span>
                    <span class="n">request</span><span class="o">=</span><span class="n">httpx</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;completion&quot;</span><span class="p">,</span> <span class="n">url</span><span class="o">=</span><span class="s2">&quot;https://github.com/BerriAI/litellm&quot;</span><span class="p">),</span>  <span class="c1"># type: ignore</span>
                <span class="p">),</span>
                <span class="n">llm_provider</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
            <span class="p">)</span></div>



<div class="viewcode-block" id="acompletion">
<a class="viewcode-back" href="../../../../source/notdiamond.toolkit.litellm.html#notdiamond.toolkit.litellm.main.acompletion">[docs]</a>
<span class="nd">@client</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">acompletion</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="c1"># Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create</span>
    <span class="n">messages</span><span class="p">:</span> <span class="n">List</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="n">functions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">function_call</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">timeout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stream</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stream_options</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">presence_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">frequency_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">logit_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">user</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="c1"># openai v1.0+ new params</span>
    <span class="n">response_format</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="n">Type</span><span class="p">[</span><span class="n">BaseModel</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tools</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tool_choice</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">parallel_tool_calls</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">logprobs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_logprobs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">deployment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="c1"># set api_base, api_version, api_key</span>
    <span class="n">base_url</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">api_version</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">api_key</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">model_list</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># pass in a list of api_base,keys, etc.</span>
    <span class="n">extra_headers</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="c1"># Optional liteLLM function params</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">ModelResponse</span><span class="p">,</span> <span class="n">CustomStreamWrapper</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Asynchronously executes a litellm.completion() call for any of litellm supported llms (example gpt-4, gpt-3.5-turbo, claude-2, command-nightly)</span>

<span class="sd">    Parameters:</span>
<span class="sd">        model (str): The name of the language model to use for text completion. see all supported LLMs: https://docs.litellm.ai/docs/providers/</span>
<span class="sd">        messages (List): A list of message objects representing the conversation context (default is an empty list).</span>

<span class="sd">        OPTIONAL PARAMS</span>
<span class="sd">        functions (List, optional): A list of functions to apply to the conversation messages (default is an empty list).</span>
<span class="sd">        function_call (str, optional): The name of the function to call within the conversation (default is an empty string).</span>
<span class="sd">        temperature (float, optional): The temperature parameter for controlling the randomness of the output (default is 1.0).</span>
<span class="sd">        top_p (float, optional): The top-p parameter for nucleus sampling (default is 1.0).</span>
<span class="sd">        n (int, optional): The number of completions to generate (default is 1).</span>
<span class="sd">        stream (bool, optional): If True, return a streaming response (default is False).</span>
<span class="sd">        stream_options (dict, optional): A dictionary containing options for the streaming response. Only use this if stream is True.</span>
<span class="sd">        stop(string/list, optional): - Up to 4 sequences where the LLM API will stop generating further tokens.</span>
<span class="sd">        max_tokens (integer, optional): The maximum number of tokens in the generated completion (default is infinity).</span>
<span class="sd">        presence_penalty (float, optional): It is used to penalize new tokens based on their existence in the text so far.</span>
<span class="sd">        frequency_penalty: It is used to penalize new tokens based on their frequency in the text so far.</span>
<span class="sd">        logit_bias (dict, optional): Used to modify the probability of specific tokens appearing in the completion.</span>
<span class="sd">        user (str, optional):  A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse.</span>
<span class="sd">        metadata (dict, optional): Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc.</span>
<span class="sd">        api_base (str, optional): Base URL for the API (default is None).</span>
<span class="sd">        api_version (str, optional): API version (default is None).</span>
<span class="sd">        api_key (str, optional): API key (default is None).</span>
<span class="sd">        model_list (list, optional): List of api base, version, keys</span>
<span class="sd">        timeout (float, optional): The maximum execution time in seconds for the completion request.</span>

<span class="sd">        LITELLM Specific Params</span>
<span class="sd">        mock_response (str, optional): If provided, return a mock completion response for testing or debugging purposes (default is None).</span>
<span class="sd">        custom_llm_provider (str, optional): Used for Non-OpenAI LLMs, Example usage for bedrock, set model=&quot;amazon.titan-tg1-large&quot; and custom_llm_provider=&quot;bedrock&quot;</span>
<span class="sd">    Returns:</span>
<span class="sd">        ModelResponse: A response object containing the generated completion and associated metadata.</span>

<span class="sd">    Notes:</span>
<span class="sd">        - This function is an asynchronous version of the `completion` function.</span>
<span class="sd">        - The `completion` function is called using `run_in_executor` to execute synchronously in the event loop.</span>
<span class="sd">        - If `stream` is True, the function returns an async generator that yields completion lines.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">loop</span> <span class="o">=</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">get_event_loop</span><span class="p">()</span>
    <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;custom_llm_provider&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="c1"># Adjusted to use explicit arguments instead of *args and **kwargs</span>
    <span class="n">completion_kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">model</span><span class="p">,</span>
        <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">messages</span><span class="p">,</span>
        <span class="s2">&quot;functions&quot;</span><span class="p">:</span> <span class="n">functions</span><span class="p">,</span>
        <span class="s2">&quot;function_call&quot;</span><span class="p">:</span> <span class="n">function_call</span><span class="p">,</span>
        <span class="s2">&quot;timeout&quot;</span><span class="p">:</span> <span class="n">timeout</span><span class="p">,</span>
        <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="n">temperature</span><span class="p">,</span>
        <span class="s2">&quot;top_p&quot;</span><span class="p">:</span> <span class="n">top_p</span><span class="p">,</span>
        <span class="s2">&quot;n&quot;</span><span class="p">:</span> <span class="n">n</span><span class="p">,</span>
        <span class="s2">&quot;stream&quot;</span><span class="p">:</span> <span class="n">stream</span><span class="p">,</span>
        <span class="s2">&quot;stream_options&quot;</span><span class="p">:</span> <span class="n">stream_options</span><span class="p">,</span>
        <span class="s2">&quot;stop&quot;</span><span class="p">:</span> <span class="n">stop</span><span class="p">,</span>
        <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="n">max_tokens</span><span class="p">,</span>
        <span class="s2">&quot;presence_penalty&quot;</span><span class="p">:</span> <span class="n">presence_penalty</span><span class="p">,</span>
        <span class="s2">&quot;frequency_penalty&quot;</span><span class="p">:</span> <span class="n">frequency_penalty</span><span class="p">,</span>
        <span class="s2">&quot;logit_bias&quot;</span><span class="p">:</span> <span class="n">logit_bias</span><span class="p">,</span>
        <span class="s2">&quot;user&quot;</span><span class="p">:</span> <span class="n">user</span><span class="p">,</span>
        <span class="s2">&quot;response_format&quot;</span><span class="p">:</span> <span class="n">response_format</span><span class="p">,</span>
        <span class="s2">&quot;seed&quot;</span><span class="p">:</span> <span class="n">seed</span><span class="p">,</span>
        <span class="s2">&quot;tools&quot;</span><span class="p">:</span> <span class="n">tools</span><span class="p">,</span>
        <span class="s2">&quot;tool_choice&quot;</span><span class="p">:</span> <span class="n">tool_choice</span><span class="p">,</span>
        <span class="s2">&quot;parallel_tool_calls&quot;</span><span class="p">:</span> <span class="n">parallel_tool_calls</span><span class="p">,</span>
        <span class="s2">&quot;logprobs&quot;</span><span class="p">:</span> <span class="n">logprobs</span><span class="p">,</span>
        <span class="s2">&quot;top_logprobs&quot;</span><span class="p">:</span> <span class="n">top_logprobs</span><span class="p">,</span>
        <span class="s2">&quot;deployment_id&quot;</span><span class="p">:</span> <span class="n">deployment_id</span><span class="p">,</span>
        <span class="s2">&quot;base_url&quot;</span><span class="p">:</span> <span class="n">base_url</span><span class="p">,</span>
        <span class="s2">&quot;api_version&quot;</span><span class="p">:</span> <span class="n">api_version</span><span class="p">,</span>
        <span class="s2">&quot;api_key&quot;</span><span class="p">:</span> <span class="n">api_key</span><span class="p">,</span>
        <span class="s2">&quot;model_list&quot;</span><span class="p">:</span> <span class="n">model_list</span><span class="p">,</span>
        <span class="s2">&quot;extra_headers&quot;</span><span class="p">:</span> <span class="n">extra_headers</span><span class="p">,</span>
        <span class="s2">&quot;acompletion&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>  <span class="c1"># assuming this is a required parameter</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="n">custom_llm_provider</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">custom_llm_provider</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">get_llm_provider</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">api_base</span><span class="o">=</span><span class="n">completion_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;base_url&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Use a partial function to pass your keyword arguments</span>
        <span class="n">func</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">completion</span><span class="p">,</span> <span class="o">**</span><span class="n">completion_kwargs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Add the context to the function</span>
        <span class="n">ctx</span> <span class="o">=</span> <span class="n">contextvars</span><span class="o">.</span><span class="n">copy_context</span><span class="p">()</span>
        <span class="n">func_with_context</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">run</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;openai&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;azure&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;azure_text&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;custom_openai&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;anyscale&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;mistral&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;openrouter&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;deepinfra&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;perplexity&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;groq&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;nvidia_nim&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;cerebras&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;ai21_chat&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;volcengine&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;codestral&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;text-completion-codestral&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;deepseek&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;text-completion-openai&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;huggingface&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;ollama&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;ollama_chat&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;replicate&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;vertex_ai&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;vertex_ai_beta&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;gemini&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;sagemaker&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;sagemaker_chat&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;anthropic&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;predibase&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;bedrock&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;databricks&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;triton&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;clarifai&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;watsonx&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;notdiamond&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">openai_compatible_providers</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">_custom_providers</span>
        <span class="p">):</span>  <span class="c1"># currently implemented aiohttp calls for just azure, openai, hf, ollama, vertex ai soon all.</span>
            <span class="n">init_response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">loop</span><span class="o">.</span><span class="n">run_in_executor</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">func_with_context</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">init_response</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span>
                <span class="n">init_response</span><span class="p">,</span> <span class="n">ModelResponse</span>
            <span class="p">):</span>  <span class="c1">## CACHING SCENARIO</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">init_response</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                    <span class="n">response</span> <span class="o">=</span> <span class="n">ModelResponse</span><span class="p">(</span><span class="o">**</span><span class="n">init_response</span><span class="p">)</span>
                <span class="n">response</span> <span class="o">=</span> <span class="n">init_response</span>
            <span class="k">elif</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">iscoroutine</span><span class="p">(</span><span class="n">init_response</span><span class="p">):</span>
                <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">init_response</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">response</span> <span class="o">=</span> <span class="n">init_response</span>  <span class="c1"># type: ignore</span>

            <span class="k">if</span> <span class="p">(</span>
                <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;text-completion-openai&quot;</span>
                <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;text-completion-codestral&quot;</span>
            <span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">TextCompletionResponse</span><span class="p">):</span>
                <span class="n">response</span> <span class="o">=</span> <span class="n">litellm</span><span class="o">.</span><span class="n">OpenAITextCompletionConfig</span><span class="p">()</span><span class="o">.</span><span class="n">convert_to_chat_model_response_object</span><span class="p">(</span>
                    <span class="n">response_object</span><span class="o">=</span><span class="n">response</span><span class="p">,</span>
                    <span class="n">model_response_object</span><span class="o">=</span><span class="n">litellm</span><span class="o">.</span><span class="n">ModelResponse</span><span class="p">(),</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Call the synchronous function using run_in_executor</span>
            <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">loop</span><span class="o">.</span><span class="n">run_in_executor</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">func_with_context</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">CustomStreamWrapper</span><span class="p">):</span>
            <span class="n">response</span><span class="o">.</span><span class="n">set_logging_event_loop</span><span class="p">(</span>
                <span class="n">loop</span><span class="o">=</span><span class="n">loop</span>
            <span class="p">)</span>  <span class="c1"># sets the logging event loop if the user does sync streaming (e.g. on proxy for sagemaker calls)</span>
        <span class="k">return</span> <span class="n">response</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="n">custom_llm_provider</span> <span class="ow">or</span> <span class="s2">&quot;openai&quot;</span>
        <span class="k">raise</span> <span class="n">exception_type</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">custom_llm_provider</span><span class="o">=</span><span class="n">custom_llm_provider</span><span class="p">,</span>
            <span class="n">original_exception</span><span class="o">=</span><span class="n">e</span><span class="p">,</span>
            <span class="n">completion_kwargs</span><span class="o">=</span><span class="n">completion_kwargs</span><span class="p">,</span>
            <span class="n">extra_kwargs</span><span class="o">=</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span></div>



<div class="viewcode-block" id="completion">
<a class="viewcode-back" href="../../../../source/notdiamond.toolkit.litellm.html#notdiamond.toolkit.litellm.main.completion">[docs]</a>
<span class="nd">@client</span>
<span class="k">def</span> <span class="nf">completion</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="c1"># Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create</span>
    <span class="n">messages</span><span class="p">:</span> <span class="n">List</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="n">timeout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">httpx</span><span class="o">.</span><span class="n">Timeout</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stream</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stream_options</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">presence_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">frequency_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">logit_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">user</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="c1"># openai v1.0+ new params</span>
    <span class="n">response_format</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="n">Type</span><span class="p">[</span><span class="n">BaseModel</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tools</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tool_choice</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">logprobs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_logprobs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">parallel_tool_calls</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">deployment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">extra_headers</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="c1"># soon to be deprecated params by OpenAI</span>
    <span class="n">functions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">function_call</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="c1"># set api_base, api_version, api_key</span>
    <span class="n">base_url</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">api_version</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">api_key</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">model_list</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># pass in a list of api_base,keys, etc.</span>
    <span class="c1"># Optional liteLLM function params</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">ModelResponse</span><span class="p">,</span> <span class="n">CustomStreamWrapper</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Perform a completion() using any of litellm supported llms (example gpt-4, gpt-3.5-turbo, claude-2, command-nightly)</span>
<span class="sd">    Parameters:</span>
<span class="sd">        model (str): The name of the language model to use for text completion. see all supported LLMs: https://docs.litellm.ai/docs/providers/</span>
<span class="sd">        messages (List): A list of message objects representing the conversation context (default is an empty list).</span>

<span class="sd">    OPTIONAL PARAMS</span>
<span class="sd">        functions (List, optional): A list of functions to apply to the conversation messages (default is an empty list).</span>
<span class="sd">        function_call (str, optional): The name of the function to call within the conversation (default is an empty string).</span>
<span class="sd">        temperature (float, optional): The temperature parameter for controlling the randomness of the output (default is 1.0).</span>
<span class="sd">        top_p (float, optional): The top-p parameter for nucleus sampling (default is 1.0).</span>
<span class="sd">        n (int, optional): The number of completions to generate (default is 1).</span>
<span class="sd">        stream (bool, optional): If True, return a streaming response (default is False).</span>
<span class="sd">        stream_options (dict, optional): A dictionary containing options for the streaming response. Only set this when you set stream: true.</span>
<span class="sd">        stop(string/list, optional): - Up to 4 sequences where the LLM API will stop generating further tokens.</span>
<span class="sd">        max_tokens (integer, optional): The maximum number of tokens in the generated completion (default is infinity).</span>
<span class="sd">        presence_penalty (float, optional): It is used to penalize new tokens based on their existence in the text so far.</span>
<span class="sd">        frequency_penalty: It is used to penalize new tokens based on their frequency in the text so far.</span>
<span class="sd">        logit_bias (dict, optional): Used to modify the probability of specific tokens appearing in the completion.</span>
<span class="sd">        user (str, optional):  A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse.</span>
<span class="sd">        logprobs (bool, optional): Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message</span>
<span class="sd">        top_logprobs (int, optional): An integer between 0 and 5 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.</span>
<span class="sd">        metadata (dict, optional): Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc.</span>
<span class="sd">        api_base (str, optional): Base URL for the API (default is None).</span>
<span class="sd">        api_version (str, optional): API version (default is None).</span>
<span class="sd">        api_key (str, optional): API key (default is None).</span>
<span class="sd">        model_list (list, optional): List of api base, version, keys</span>
<span class="sd">        extra_headers (dict, optional): Additional headers to include in the request.</span>

<span class="sd">    LITELLM Specific Params</span>
<span class="sd">        mock_response (str, optional): If provided, return a mock completion response for testing or debugging purposes (default is None).</span>
<span class="sd">        custom_llm_provider (str, optional): Used for Non-OpenAI LLMs, Example usage for bedrock, set model=&quot;amazon.titan-tg1-large&quot; and custom_llm_provider=&quot;bedrock&quot;</span>
<span class="sd">        max_retries (int, optional): The number of retries to attempt (default is 0).</span>

<span class="sd">    Returns:</span>
<span class="sd">        ModelResponse: A response object containing the generated completion and associated metadata.</span>

<span class="sd">    Note:</span>
<span class="sd">        - This function is used to perform completions() using the specified language model.</span>
<span class="sd">        - It supports various optional parameters for customizing the completion behavior.</span>
<span class="sd">        - If &#39;mock_response&#39; is provided, a mock completion response is returned for testing or debugging.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">######### unpacking kwargs #####################</span>
    <span class="n">args</span> <span class="o">=</span> <span class="nb">locals</span><span class="p">()</span>
    <span class="n">api_base</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;api_base&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">mock_response</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;mock_response&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">mock_tool_calls</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;mock_tool_calls&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">force_timeout</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;force_timeout&quot;</span><span class="p">,</span> <span class="mi">600</span><span class="p">)</span>  <span class="c1">## deprecated</span>
    <span class="n">logger_fn</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;logger_fn&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">verbose</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;verbose&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;custom_llm_provider&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">litellm_logging_obj</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;litellm_logging_obj&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="nb">id</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">metadata</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;metadata&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;model_info&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">proxy_server_request</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;proxy_server_request&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">fallbacks</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;fallbacks&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">headers</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;headers&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="n">extra_headers</span>
    <span class="n">num_retries</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
        <span class="s2">&quot;num_retries&quot;</span><span class="p">,</span> <span class="kc">None</span>
    <span class="p">)</span>  <span class="c1">## alt. param for &#39;max_retries&#39;. Use this to pass retries w/ instructor.</span>
    <span class="n">max_retries</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;max_retries&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">cooldown_time</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;cooldown_time&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">context_window_fallback_dict</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
        <span class="s2">&quot;context_window_fallback_dict&quot;</span><span class="p">,</span> <span class="kc">None</span>
    <span class="p">)</span>
    <span class="n">organization</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;organization&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="c1">### CUSTOM MODEL COST ###</span>
    <span class="n">input_cost_per_token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;input_cost_per_token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">output_cost_per_token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;output_cost_per_token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">input_cost_per_second</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;input_cost_per_second&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">output_cost_per_second</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;output_cost_per_second&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="c1">### CUSTOM PROMPT TEMPLATE ###</span>
    <span class="n">initial_prompt_value</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;initial_prompt_value&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">roles</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;roles&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">final_prompt_value</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;final_prompt_value&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">bos_token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;bos_token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">eos_token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;eos_token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">preset_cache_key</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;preset_cache_key&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">hf_model_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;hf_model_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">supports_system_message</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;supports_system_message&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="c1">### TEXT COMPLETION CALLS ###</span>
    <span class="n">text_completion</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;text_completion&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">atext_completion</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;atext_completion&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="c1">### ASYNC CALLS ###</span>
    <span class="n">acompletion</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;acompletion&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">client</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;client&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="c1">### Admin Controls ###</span>
    <span class="n">no_log</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;no-log&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="c1">### COPY MESSAGES ### - related issue https://github.com/BerriAI/litellm/discussions/4489</span>
    <span class="n">messages</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
    <span class="c1">######## end of unpacking kwargs ###########</span>
    <span class="n">openai_params</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;functions&quot;</span><span class="p">,</span>
        <span class="s2">&quot;function_call&quot;</span><span class="p">,</span>
        <span class="s2">&quot;temperature&quot;</span><span class="p">,</span>
        <span class="s2">&quot;temperature&quot;</span><span class="p">,</span>
        <span class="s2">&quot;top_p&quot;</span><span class="p">,</span>
        <span class="s2">&quot;n&quot;</span><span class="p">,</span>
        <span class="s2">&quot;stream&quot;</span><span class="p">,</span>
        <span class="s2">&quot;stream_options&quot;</span><span class="p">,</span>
        <span class="s2">&quot;stop&quot;</span><span class="p">,</span>
        <span class="s2">&quot;max_tokens&quot;</span><span class="p">,</span>
        <span class="s2">&quot;presence_penalty&quot;</span><span class="p">,</span>
        <span class="s2">&quot;frequency_penalty&quot;</span><span class="p">,</span>
        <span class="s2">&quot;logit_bias&quot;</span><span class="p">,</span>
        <span class="s2">&quot;user&quot;</span><span class="p">,</span>
        <span class="s2">&quot;request_timeout&quot;</span><span class="p">,</span>
        <span class="s2">&quot;api_base&quot;</span><span class="p">,</span>
        <span class="s2">&quot;api_version&quot;</span><span class="p">,</span>
        <span class="s2">&quot;api_key&quot;</span><span class="p">,</span>
        <span class="s2">&quot;deployment_id&quot;</span><span class="p">,</span>
        <span class="s2">&quot;organization&quot;</span><span class="p">,</span>
        <span class="s2">&quot;base_url&quot;</span><span class="p">,</span>
        <span class="s2">&quot;default_headers&quot;</span><span class="p">,</span>
        <span class="s2">&quot;timeout&quot;</span><span class="p">,</span>
        <span class="s2">&quot;response_format&quot;</span><span class="p">,</span>
        <span class="s2">&quot;seed&quot;</span><span class="p">,</span>
        <span class="s2">&quot;tools&quot;</span><span class="p">,</span>
        <span class="s2">&quot;tool_choice&quot;</span><span class="p">,</span>
        <span class="s2">&quot;max_retries&quot;</span><span class="p">,</span>
        <span class="s2">&quot;parallel_tool_calls&quot;</span><span class="p">,</span>
        <span class="s2">&quot;logprobs&quot;</span><span class="p">,</span>
        <span class="s2">&quot;top_logprobs&quot;</span><span class="p">,</span>
        <span class="s2">&quot;extra_headers&quot;</span><span class="p">,</span>
    <span class="p">]</span>
    <span class="n">litellm_params</span> <span class="o">=</span> <span class="n">all_litellm_params</span>  <span class="c1"># use the external var., used in creating cache key as well.</span>

    <span class="n">default_params</span> <span class="o">=</span> <span class="n">openai_params</span> <span class="o">+</span> <span class="n">litellm_params</span>
    <span class="n">non_default_params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">default_params</span>
    <span class="p">}</span>  <span class="c1"># model-specific params - pass them straight to the model/provider</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">base_url</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">api_base</span> <span class="o">=</span> <span class="n">base_url</span>
        <span class="k">if</span> <span class="n">num_retries</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">max_retries</span> <span class="o">=</span> <span class="n">num_retries</span>
        <span class="n">logging</span> <span class="o">=</span> <span class="n">litellm_logging_obj</span>
        <span class="n">fallbacks</span> <span class="o">=</span> <span class="n">fallbacks</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">model_fallbacks</span>
        <span class="k">if</span> <span class="n">fallbacks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">completion_with_fallbacks</span><span class="p">(</span><span class="o">**</span><span class="n">args</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model_list</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">deployments</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">m</span><span class="p">[</span><span class="s2">&quot;litellm_params&quot;</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">model_list</span>
                <span class="k">if</span> <span class="n">m</span><span class="p">[</span><span class="s2">&quot;model_name&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">model</span>
            <span class="p">]</span>
            <span class="k">return</span> <span class="n">batch_completion_models</span><span class="p">(</span><span class="n">deployments</span><span class="o">=</span><span class="n">deployments</span><span class="p">,</span> <span class="o">**</span><span class="n">args</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">litellm</span><span class="o">.</span><span class="n">model_alias_map</span> <span class="ow">and</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">model_alias_map</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">litellm</span><span class="o">.</span><span class="n">model_alias_map</span><span class="p">[</span>
                <span class="n">model</span>
            <span class="p">]</span>  <span class="c1"># update the model to the actual value if an alias has been passed in</span>
        <span class="n">model_response</span> <span class="o">=</span> <span class="n">ModelResponse</span><span class="p">()</span>
        <span class="nb">setattr</span><span class="p">(</span><span class="n">model_response</span><span class="p">,</span> <span class="s2">&quot;usage&quot;</span><span class="p">,</span> <span class="n">litellm</span><span class="o">.</span><span class="n">Usage</span><span class="p">())</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;azure&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="o">==</span> <span class="kc">True</span>
        <span class="p">):</span>  <span class="c1"># don&#39;t remove flag check, to remain backwards compatible for repos like Codium</span>
            <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;azure&quot;</span>
        <span class="k">if</span> <span class="n">deployment_id</span> <span class="o">!=</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># azure llms</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">deployment_id</span>
            <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;azure&quot;</span>
        <span class="p">(</span>
            <span class="n">model</span><span class="p">,</span>
            <span class="n">custom_llm_provider</span><span class="p">,</span>
            <span class="n">dynamic_api_key</span><span class="p">,</span>
            <span class="n">api_base</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="n">get_llm_provider</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">custom_llm_provider</span><span class="o">=</span><span class="n">custom_llm_provider</span><span class="p">,</span>
            <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
            <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">model_response</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span>
            <span class="n">model_response</span><span class="p">,</span> <span class="s2">&quot;_hidden_params&quot;</span>
        <span class="p">):</span>
            <span class="n">model_response</span><span class="o">.</span><span class="n">_hidden_params</span><span class="p">[</span>
                <span class="s2">&quot;custom_llm_provider&quot;</span>
            <span class="p">]</span> <span class="o">=</span> <span class="n">custom_llm_provider</span>
            <span class="n">model_response</span><span class="o">.</span><span class="n">_hidden_params</span><span class="p">[</span><span class="s2">&quot;region_name&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                <span class="s2">&quot;aws_region_name&quot;</span><span class="p">,</span> <span class="kc">None</span>
            <span class="p">)</span>  <span class="c1"># support region-based pricing for bedrock</span>

        <span class="c1">### TIMEOUT LOGIC ###</span>
        <span class="n">timeout</span> <span class="o">=</span> <span class="n">timeout</span> <span class="ow">or</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;request_timeout&quot;</span><span class="p">,</span> <span class="mi">600</span><span class="p">)</span> <span class="ow">or</span> <span class="mi">600</span>
        <span class="c1"># set timeout for 10 minutes by default</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">timeout</span><span class="p">,</span> <span class="n">httpx</span><span class="o">.</span><span class="n">Timeout</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">supports_httpx_timeout</span><span class="p">(</span>
            <span class="n">custom_llm_provider</span>
        <span class="p">):</span>
            <span class="n">timeout</span> <span class="o">=</span> <span class="n">timeout</span><span class="o">.</span><span class="n">read</span> <span class="ow">or</span> <span class="mi">600</span>  <span class="c1"># default 10 min timeout</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">timeout</span><span class="p">,</span> <span class="n">httpx</span><span class="o">.</span><span class="n">Timeout</span><span class="p">):</span>
            <span class="n">timeout</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">timeout</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

        <span class="c1">### REGISTER CUSTOM MODEL PRICING -- IF GIVEN ###</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">input_cost_per_token</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="n">output_cost_per_token</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="n">litellm</span><span class="o">.</span><span class="n">register_model</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">custom_llm_provider</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="s2">&quot;input_cost_per_token&quot;</span><span class="p">:</span> <span class="n">input_cost_per_token</span><span class="p">,</span>
                        <span class="s2">&quot;output_cost_per_token&quot;</span><span class="p">:</span> <span class="n">output_cost_per_token</span><span class="p">,</span>
                        <span class="s2">&quot;litellm_provider&quot;</span><span class="p">:</span> <span class="n">custom_llm_provider</span><span class="p">,</span>
                    <span class="p">},</span>
                    <span class="n">model</span><span class="p">:</span> <span class="p">{</span>
                        <span class="s2">&quot;input_cost_per_token&quot;</span><span class="p">:</span> <span class="n">input_cost_per_token</span><span class="p">,</span>
                        <span class="s2">&quot;output_cost_per_token&quot;</span><span class="p">:</span> <span class="n">output_cost_per_token</span><span class="p">,</span>
                        <span class="s2">&quot;litellm_provider&quot;</span><span class="p">:</span> <span class="n">custom_llm_provider</span><span class="p">,</span>
                    <span class="p">},</span>
                <span class="p">}</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="p">(</span>
            <span class="n">input_cost_per_second</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">):</span>  <span class="c1"># time based pricing just needs cost in place</span>
            <span class="n">output_cost_per_second</span> <span class="o">=</span> <span class="n">output_cost_per_second</span>
            <span class="n">litellm</span><span class="o">.</span><span class="n">register_model</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">custom_llm_provider</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="s2">&quot;input_cost_per_second&quot;</span><span class="p">:</span> <span class="n">input_cost_per_second</span><span class="p">,</span>
                        <span class="s2">&quot;output_cost_per_second&quot;</span><span class="p">:</span> <span class="n">output_cost_per_second</span><span class="p">,</span>
                        <span class="s2">&quot;litellm_provider&quot;</span><span class="p">:</span> <span class="n">custom_llm_provider</span><span class="p">,</span>
                    <span class="p">},</span>
                    <span class="n">model</span><span class="p">:</span> <span class="p">{</span>
                        <span class="s2">&quot;input_cost_per_second&quot;</span><span class="p">:</span> <span class="n">input_cost_per_second</span><span class="p">,</span>
                        <span class="s2">&quot;output_cost_per_second&quot;</span><span class="p">:</span> <span class="n">output_cost_per_second</span><span class="p">,</span>
                        <span class="s2">&quot;litellm_provider&quot;</span><span class="p">:</span> <span class="n">custom_llm_provider</span><span class="p">,</span>
                    <span class="p">},</span>
                <span class="p">}</span>
            <span class="p">)</span>
        <span class="c1">### BUILD CUSTOM PROMPT TEMPLATE -- IF GIVEN ###</span>
        <span class="n">custom_prompt_dict</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># type: ignore</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">initial_prompt_value</span>
            <span class="ow">or</span> <span class="n">roles</span>
            <span class="ow">or</span> <span class="n">final_prompt_value</span>
            <span class="ow">or</span> <span class="n">bos_token</span>
            <span class="ow">or</span> <span class="n">eos_token</span>
        <span class="p">):</span>
            <span class="n">custom_prompt_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">model</span><span class="p">:</span> <span class="p">{}}</span>
            <span class="k">if</span> <span class="n">initial_prompt_value</span><span class="p">:</span>
                <span class="n">custom_prompt_dict</span><span class="p">[</span><span class="n">model</span><span class="p">][</span>
                    <span class="s2">&quot;initial_prompt_value&quot;</span>
                <span class="p">]</span> <span class="o">=</span> <span class="n">initial_prompt_value</span>
            <span class="k">if</span> <span class="n">roles</span><span class="p">:</span>
                <span class="n">custom_prompt_dict</span><span class="p">[</span><span class="n">model</span><span class="p">][</span><span class="s2">&quot;roles&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">roles</span>
            <span class="k">if</span> <span class="n">final_prompt_value</span><span class="p">:</span>
                <span class="n">custom_prompt_dict</span><span class="p">[</span><span class="n">model</span><span class="p">][</span>
                    <span class="s2">&quot;final_prompt_value&quot;</span>
                <span class="p">]</span> <span class="o">=</span> <span class="n">final_prompt_value</span>
            <span class="k">if</span> <span class="n">bos_token</span><span class="p">:</span>
                <span class="n">custom_prompt_dict</span><span class="p">[</span><span class="n">model</span><span class="p">][</span><span class="s2">&quot;bos_token&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">bos_token</span>
            <span class="k">if</span> <span class="n">eos_token</span><span class="p">:</span>
                <span class="n">custom_prompt_dict</span><span class="p">[</span><span class="n">model</span><span class="p">][</span><span class="s2">&quot;eos_token&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">eos_token</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="n">supports_system_message</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">supports_system_message</span><span class="p">,</span> <span class="nb">bool</span><span class="p">)</span>
            <span class="ow">and</span> <span class="n">supports_system_message</span> <span class="ow">is</span> <span class="kc">False</span>
        <span class="p">):</span>
            <span class="n">messages</span> <span class="o">=</span> <span class="n">map_system_message_pt</span><span class="p">(</span><span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">)</span>
        <span class="n">model_api_key</span> <span class="o">=</span> <span class="n">get_api_key</span><span class="p">(</span>
            <span class="n">llm_provider</span><span class="o">=</span><span class="n">custom_llm_provider</span><span class="p">,</span> <span class="n">dynamic_api_key</span><span class="o">=</span><span class="n">api_key</span>
        <span class="p">)</span>  <span class="c1"># get the api key from the environment if required for the model</span>

        <span class="k">if</span> <span class="n">dynamic_api_key</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">api_key</span> <span class="o">=</span> <span class="n">dynamic_api_key</span>
        <span class="c1"># check if user passed in any of the OpenAI optional params</span>
        <span class="n">optional_params</span> <span class="o">=</span> <span class="n">get_optional_params</span><span class="p">(</span>
            <span class="n">functions</span><span class="o">=</span><span class="n">functions</span><span class="p">,</span>
            <span class="n">function_call</span><span class="o">=</span><span class="n">function_call</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
            <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span>
            <span class="n">stream</span><span class="o">=</span><span class="n">stream</span><span class="p">,</span>
            <span class="n">stream_options</span><span class="o">=</span><span class="n">stream_options</span><span class="p">,</span>
            <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span>
            <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_tokens</span><span class="p">,</span>
            <span class="n">presence_penalty</span><span class="o">=</span><span class="n">presence_penalty</span><span class="p">,</span>
            <span class="n">frequency_penalty</span><span class="o">=</span><span class="n">frequency_penalty</span><span class="p">,</span>
            <span class="n">logit_bias</span><span class="o">=</span><span class="n">logit_bias</span><span class="p">,</span>
            <span class="n">user</span><span class="o">=</span><span class="n">user</span><span class="p">,</span>
            <span class="c1"># params to identify the model</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">custom_llm_provider</span><span class="o">=</span><span class="n">custom_llm_provider</span><span class="p">,</span>
            <span class="n">response_format</span><span class="o">=</span><span class="n">response_format</span><span class="p">,</span>
            <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
            <span class="n">tools</span><span class="o">=</span><span class="n">tools</span><span class="p">,</span>
            <span class="n">tool_choice</span><span class="o">=</span><span class="n">tool_choice</span><span class="p">,</span>
            <span class="n">max_retries</span><span class="o">=</span><span class="n">max_retries</span><span class="p">,</span>
            <span class="n">logprobs</span><span class="o">=</span><span class="n">logprobs</span><span class="p">,</span>
            <span class="n">top_logprobs</span><span class="o">=</span><span class="n">top_logprobs</span><span class="p">,</span>
            <span class="n">extra_headers</span><span class="o">=</span><span class="n">extra_headers</span><span class="p">,</span>
            <span class="n">api_version</span><span class="o">=</span><span class="n">api_version</span><span class="p">,</span>
            <span class="n">parallel_tool_calls</span><span class="o">=</span><span class="n">parallel_tool_calls</span><span class="p">,</span>
            <span class="o">**</span><span class="n">non_default_params</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">litellm</span><span class="o">.</span><span class="n">add_function_to_prompt</span> <span class="ow">and</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">&quot;functions_unsupported_model&quot;</span><span class="p">,</span> <span class="kc">None</span>
        <span class="p">):</span>  <span class="c1"># if user opts to add it to prompt, when API doesn&#39;t support function calling</span>
            <span class="n">functions_unsupported_model</span> <span class="o">=</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span>
                <span class="s2">&quot;functions_unsupported_model&quot;</span>
            <span class="p">)</span>
            <span class="n">messages</span> <span class="o">=</span> <span class="n">function_call_prompt</span><span class="p">(</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span> <span class="n">functions</span><span class="o">=</span><span class="n">functions_unsupported_model</span>
            <span class="p">)</span>

        <span class="c1"># For logging - save the values of the litellm-specific params passed in</span>
        <span class="n">litellm_params</span> <span class="o">=</span> <span class="n">get_litellm_params</span><span class="p">(</span>
            <span class="n">acompletion</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span>
            <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
            <span class="n">force_timeout</span><span class="o">=</span><span class="n">force_timeout</span><span class="p">,</span>
            <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">custom_llm_provider</span><span class="o">=</span><span class="n">custom_llm_provider</span><span class="p">,</span>
            <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
            <span class="n">litellm_call_id</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;litellm_call_id&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
            <span class="n">model_alias_map</span><span class="o">=</span><span class="n">litellm</span><span class="o">.</span><span class="n">model_alias_map</span><span class="p">,</span>
            <span class="n">completion_call_id</span><span class="o">=</span><span class="nb">id</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">model_info</span><span class="o">=</span><span class="n">model_info</span><span class="p">,</span>
            <span class="n">proxy_server_request</span><span class="o">=</span><span class="n">proxy_server_request</span><span class="p">,</span>
            <span class="n">preset_cache_key</span><span class="o">=</span><span class="n">preset_cache_key</span><span class="p">,</span>
            <span class="n">no_log</span><span class="o">=</span><span class="n">no_log</span><span class="p">,</span>
            <span class="n">input_cost_per_second</span><span class="o">=</span><span class="n">input_cost_per_second</span><span class="p">,</span>
            <span class="n">input_cost_per_token</span><span class="o">=</span><span class="n">input_cost_per_token</span><span class="p">,</span>
            <span class="n">output_cost_per_second</span><span class="o">=</span><span class="n">output_cost_per_second</span><span class="p">,</span>
            <span class="n">output_cost_per_token</span><span class="o">=</span><span class="n">output_cost_per_token</span><span class="p">,</span>
            <span class="n">cooldown_time</span><span class="o">=</span><span class="n">cooldown_time</span><span class="p">,</span>
            <span class="n">text_completion</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;text_completion&quot;</span><span class="p">),</span>
            <span class="n">azure_ad_token_provider</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;azure_ad_token_provider&quot;</span><span class="p">),</span>
            <span class="n">user_continue_message</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;user_continue_message&quot;</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">update_environment_variables</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">user</span><span class="o">=</span><span class="n">user</span><span class="p">,</span>
            <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
            <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
            <span class="n">custom_llm_provider</span><span class="o">=</span><span class="n">custom_llm_provider</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">mock_response</span> <span class="ow">or</span> <span class="n">mock_tool_calls</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">mock_completion</span><span class="p">(</span>
                <span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="p">,</span>
                <span class="n">stream</span><span class="o">=</span><span class="n">stream</span><span class="p">,</span>
                <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span>
                <span class="n">mock_response</span><span class="o">=</span><span class="n">mock_response</span><span class="p">,</span>
                <span class="n">mock_tool_calls</span><span class="o">=</span><span class="n">mock_tool_calls</span><span class="p">,</span>
                <span class="n">logging</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="n">acompletion</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span>
                <span class="n">mock_delay</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;mock_delay&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                <span class="n">custom_llm_provider</span><span class="o">=</span><span class="n">custom_llm_provider</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;azure&quot;</span><span class="p">:</span>
            <span class="c1"># azure configs</span>
            <span class="c1">## check dynamic params ##</span>
            <span class="n">dynamic_params</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="n">client</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">client</span><span class="p">,</span> <span class="n">openai</span><span class="o">.</span><span class="n">AzureOpenAI</span><span class="p">)</span>
                <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">client</span><span class="p">,</span> <span class="n">openai</span><span class="o">.</span><span class="n">AsyncAzureOpenAI</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="n">dynamic_params</span> <span class="o">=</span> <span class="n">_check_dynamic_azure_params</span><span class="p">(</span>
                    <span class="n">azure_client_params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;api_version&quot;</span><span class="p">:</span> <span class="n">api_version</span><span class="p">},</span>
                    <span class="n">azure_client</span><span class="o">=</span><span class="n">client</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="n">api_type</span> <span class="o">=</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;AZURE_API_TYPE&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="s2">&quot;azure&quot;</span>

            <span class="n">api_base</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_base</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_base</span> <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;AZURE_API_BASE&quot;</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="n">api_version</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_version</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_version</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;AZURE_API_VERSION&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">AZURE_DEFAULT_API_VERSION</span>
            <span class="p">)</span>

            <span class="n">api_key</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">azure_key</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;AZURE_OPENAI_API_KEY&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;AZURE_API_KEY&quot;</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="n">azure_ad_token</span> <span class="o">=</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;extra_body&quot;</span><span class="p">,</span> <span class="p">{})</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span>
                <span class="s2">&quot;azure_ad_token&quot;</span><span class="p">,</span> <span class="kc">None</span>
            <span class="p">)</span> <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;AZURE_AD_TOKEN&quot;</span><span class="p">)</span>

            <span class="n">headers</span> <span class="o">=</span> <span class="n">headers</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">headers</span>

            <span class="c1">## LOAD CONFIG - if set</span>
            <span class="n">config</span> <span class="o">=</span> <span class="n">litellm</span><span class="o">.</span><span class="n">AzureOpenAIConfig</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">optional_params</span>
                <span class="p">):</span>  <span class="c1"># completion(top_k=3) &gt; azure_config(top_k=3) &lt;- allows for dynamic variables to be passed in</span>
                    <span class="n">optional_params</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>

            <span class="c1">## COMPLETION CALL</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">azure_chat_completions</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span>
                <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
                <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                <span class="n">api_version</span><span class="o">=</span><span class="n">api_version</span><span class="p">,</span>
                <span class="n">api_type</span><span class="o">=</span><span class="n">api_type</span><span class="p">,</span>
                <span class="n">dynamic_params</span><span class="o">=</span><span class="n">dynamic_params</span><span class="p">,</span>
                <span class="n">azure_ad_token</span><span class="o">=</span><span class="n">azure_ad_token</span><span class="p">,</span>
                <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="n">acompletion</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span>
                <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">client</span><span class="o">=</span><span class="n">client</span><span class="p">,</span>  <span class="c1"># pass AsyncAzureOpenAI, AzureOpenAI client</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;stream&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                <span class="c1">## LOGGING</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">post_call</span><span class="p">(</span>
                    <span class="nb">input</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
                    <span class="n">original_response</span><span class="o">=</span><span class="n">response</span><span class="p">,</span>
                    <span class="n">additional_args</span><span class="o">=</span><span class="p">{</span>
                        <span class="s2">&quot;headers&quot;</span><span class="p">:</span> <span class="n">headers</span><span class="p">,</span>
                        <span class="s2">&quot;api_version&quot;</span><span class="p">:</span> <span class="n">api_version</span><span class="p">,</span>
                        <span class="s2">&quot;api_base&quot;</span><span class="p">:</span> <span class="n">api_base</span><span class="p">,</span>
                    <span class="p">},</span>
                <span class="p">)</span>

        <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;notdiamond&quot;</span><span class="p">:</span>
            <span class="n">notdiamond_key</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">notdiamond_key</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;NOTDIAMOND_API_KEY&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_key</span>
            <span class="p">)</span>

            <span class="n">api_base</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;NOTDIAMOND_API_BASE&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="s2">&quot;https://api.notdiamond.ai/v2/optimizer/modelSelect&quot;</span>
            <span class="p">)</span>

            <span class="c1"># since notdiamond.completion() internally calls other models&#39; completion functions</span>
            <span class="c1"># streaming does not need to be handled separately</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">notdiamond_completion</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
                <span class="n">api_key</span><span class="o">=</span><span class="n">notdiamond_key</span><span class="p">,</span>
                <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;stream&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="ow">or</span> <span class="n">acompletion</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
                <span class="c1">## LOGGING</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">post_call</span><span class="p">(</span>
                    <span class="nb">input</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
                    <span class="n">original_response</span><span class="o">=</span><span class="n">response</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="n">response</span> <span class="o">=</span> <span class="n">response</span>

        <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;azure_text&quot;</span><span class="p">:</span>
            <span class="c1"># azure configs</span>
            <span class="n">api_type</span> <span class="o">=</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;AZURE_API_TYPE&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="s2">&quot;azure&quot;</span>

            <span class="n">api_base</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_base</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_base</span> <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;AZURE_API_BASE&quot;</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="n">api_version</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_version</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_version</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;AZURE_API_VERSION&quot;</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="n">api_key</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">azure_key</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;AZURE_OPENAI_API_KEY&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;AZURE_API_KEY&quot;</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="n">azure_ad_token</span> <span class="o">=</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;extra_body&quot;</span><span class="p">,</span> <span class="p">{})</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span>
                <span class="s2">&quot;azure_ad_token&quot;</span><span class="p">,</span> <span class="kc">None</span>
            <span class="p">)</span> <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;AZURE_AD_TOKEN&quot;</span><span class="p">)</span>

            <span class="n">headers</span> <span class="o">=</span> <span class="n">headers</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">headers</span>

            <span class="c1">## LOAD CONFIG - if set</span>
            <span class="n">config</span> <span class="o">=</span> <span class="n">litellm</span><span class="o">.</span><span class="n">AzureOpenAIConfig</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">optional_params</span>
                <span class="p">):</span>  <span class="c1"># completion(top_k=3) &gt; azure_config(top_k=3) &lt;- allows for dynamic variables to be passed in</span>
                    <span class="n">optional_params</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>

            <span class="c1">## COMPLETION CALL</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">azure_text_completions</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span>
                <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
                <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                <span class="n">api_version</span><span class="o">=</span><span class="n">api_version</span><span class="p">,</span>
                <span class="n">api_type</span><span class="o">=</span><span class="n">api_type</span><span class="p">,</span>
                <span class="n">azure_ad_token</span><span class="o">=</span><span class="n">azure_ad_token</span><span class="p">,</span>
                <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="n">acompletion</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span>
                <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>
                <span class="n">client</span><span class="o">=</span><span class="n">client</span><span class="p">,</span>  <span class="c1"># pass AsyncAzureOpenAI, AzureOpenAI client</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;stream&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="ow">or</span> <span class="n">acompletion</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
                <span class="c1">## LOGGING</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">post_call</span><span class="p">(</span>
                    <span class="nb">input</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
                    <span class="n">original_response</span><span class="o">=</span><span class="n">response</span><span class="p">,</span>
                    <span class="n">additional_args</span><span class="o">=</span><span class="p">{</span>
                        <span class="s2">&quot;headers&quot;</span><span class="p">:</span> <span class="n">headers</span><span class="p">,</span>
                        <span class="s2">&quot;api_version&quot;</span><span class="p">:</span> <span class="n">api_version</span><span class="p">,</span>
                        <span class="s2">&quot;api_base&quot;</span><span class="p">:</span> <span class="n">api_base</span><span class="p">,</span>
                    <span class="p">},</span>
                <span class="p">)</span>
        <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;azure_ai&quot;</span><span class="p">:</span>
            <span class="n">api_base</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_base</span>  <span class="c1"># for deepinfra/perplexity/anyscale/groq/friendliai we check in get_llm_provider and pass in the api base from there</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;AZURE_AI_API_BASE&quot;</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="c1"># set API KEY</span>
            <span class="n">api_key</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_key</span>  <span class="c1"># for deepinfra/perplexity/anyscale/friendliai we check in get_llm_provider and pass in the api key from there</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">openai_key</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;AZURE_AI_API_KEY&quot;</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="n">headers</span> <span class="o">=</span> <span class="n">headers</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">headers</span>

            <span class="c1">## LOAD CONFIG - if set</span>
            <span class="n">config</span> <span class="o">=</span> <span class="n">litellm</span><span class="o">.</span><span class="n">OpenAIConfig</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">optional_params</span>
                <span class="p">):</span>  <span class="c1"># completion(top_k=3) &gt; openai_config(top_k=3) &lt;- allows for dynamic variables to be passed in</span>
                    <span class="n">optional_params</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>

            <span class="c1">## FOR COHERE</span>
            <span class="k">if</span> <span class="s2">&quot;command-r&quot;</span> <span class="ow">in</span> <span class="n">model</span><span class="p">:</span>  <span class="c1"># make sure tool call in messages are str</span>
                <span class="n">messages</span> <span class="o">=</span> <span class="n">stringify_json_tool_call_content</span><span class="p">(</span><span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">)</span>

            <span class="c1">## COMPLETION CALL</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">response</span> <span class="o">=</span> <span class="n">openai_chat_completions</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                    <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span>
                    <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                    <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                    <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
                    <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                    <span class="n">acompletion</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span>
                    <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                    <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                    <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                    <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                    <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                    <span class="n">custom_prompt_dict</span><span class="o">=</span><span class="n">custom_prompt_dict</span><span class="p">,</span>
                    <span class="n">client</span><span class="o">=</span><span class="n">client</span><span class="p">,</span>  <span class="c1"># pass AsyncOpenAI, OpenAI client</span>
                    <span class="n">organization</span><span class="o">=</span><span class="n">organization</span><span class="p">,</span>
                    <span class="n">custom_llm_provider</span><span class="o">=</span><span class="n">custom_llm_provider</span><span class="p">,</span>
                    <span class="n">drop_params</span><span class="o">=</span><span class="n">non_default_params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;drop_params&quot;</span><span class="p">),</span>
                <span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="c1">## LOGGING - log the original exception returned</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">post_call</span><span class="p">(</span>
                    <span class="nb">input</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
                    <span class="n">original_response</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">),</span>
                    <span class="n">additional_args</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;headers&quot;</span><span class="p">:</span> <span class="n">headers</span><span class="p">},</span>
                <span class="p">)</span>
                <span class="k">raise</span> <span class="n">e</span>

            <span class="k">if</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;stream&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                <span class="c1">## LOGGING</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">post_call</span><span class="p">(</span>
                    <span class="nb">input</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
                    <span class="n">original_response</span><span class="o">=</span><span class="n">response</span><span class="p">,</span>
                    <span class="n">additional_args</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;headers&quot;</span><span class="p">:</span> <span class="n">headers</span><span class="p">},</span>
                <span class="p">)</span>
        <span class="k">elif</span> <span class="p">(</span>
            <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;text-completion-openai&quot;</span>
            <span class="ow">or</span> <span class="s2">&quot;ft:babbage-002&quot;</span> <span class="ow">in</span> <span class="n">model</span>
            <span class="ow">or</span> <span class="s2">&quot;ft:davinci-002&quot;</span>
            <span class="ow">in</span> <span class="n">model</span>  <span class="c1"># support for finetuned completion models</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span>
            <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">openai_text_completion_compatible_providers</span>
            <span class="ow">and</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;text_completion&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">True</span>
        <span class="p">):</span>
            <span class="n">openai</span><span class="o">.</span><span class="n">api_type</span> <span class="o">=</span> <span class="s2">&quot;openai&quot;</span>

            <span class="n">api_base</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;OPENAI_API_BASE&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="s2">&quot;https://api.openai.com/v1&quot;</span>
            <span class="p">)</span>

            <span class="n">openai</span><span class="o">.</span><span class="n">api_version</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="c1"># set API KEY</span>

            <span class="n">api_key</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">openai_key</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="n">headers</span> <span class="o">=</span> <span class="n">headers</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">headers</span>

            <span class="c1">## LOAD CONFIG - if set</span>
            <span class="n">config</span> <span class="o">=</span> <span class="n">litellm</span><span class="o">.</span><span class="n">OpenAITextCompletionConfig</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">optional_params</span>
                <span class="p">):</span>  <span class="c1"># completion(top_k=3) &gt; openai_text_config(top_k=3) &lt;- allows for dynamic variables to be passed in</span>
                    <span class="n">optional_params</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
            <span class="k">if</span> <span class="n">litellm</span><span class="o">.</span><span class="n">organization</span><span class="p">:</span>
                <span class="n">openai</span><span class="o">.</span><span class="n">organization</span> <span class="o">=</span> <span class="n">litellm</span><span class="o">.</span><span class="n">organization</span>

            <span class="k">if</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
                <span class="ow">and</span> <span class="s2">&quot;content&quot;</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="n">messages</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;content&quot;</span><span class="p">])</span> <span class="o">==</span> <span class="nb">list</span>
            <span class="p">):</span>
                <span class="c1"># text-davinci-003 can accept a string or array, if it&#39;s an array, assume the array is set in messages[0][&#39;content&#39;]</span>
                <span class="c1"># https://platform.openai.com/docs/api-reference/completions/create</span>
                <span class="n">prompt</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;content&quot;</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">message</span><span class="p">[</span><span class="s2">&quot;content&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">])</span>  <span class="c1"># type: ignore</span>

            <span class="c1">## COMPLETION CALL</span>
            <span class="n">_response</span> <span class="o">=</span> <span class="n">openai_text_completions</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
                <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                <span class="n">acompletion</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span>
                <span class="n">client</span><span class="o">=</span><span class="n">client</span><span class="p">,</span>  <span class="c1"># pass AsyncOpenAI, OpenAI client</span>
                <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="p">(</span>
                <span class="n">optional_params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;stream&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="o">==</span> <span class="kc">False</span>
                <span class="ow">and</span> <span class="n">acompletion</span> <span class="o">==</span> <span class="kc">False</span>
                <span class="ow">and</span> <span class="n">text_completion</span> <span class="o">==</span> <span class="kc">False</span>
            <span class="p">):</span>
                <span class="c1"># convert to chat completion response</span>
                <span class="n">_response</span> <span class="o">=</span> <span class="n">litellm</span><span class="o">.</span><span class="n">OpenAITextCompletionConfig</span><span class="p">()</span><span class="o">.</span><span class="n">convert_to_chat_model_response_object</span><span class="p">(</span>
                    <span class="n">response_object</span><span class="o">=</span><span class="n">_response</span><span class="p">,</span>
                    <span class="n">model_response_object</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;stream&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="ow">or</span> <span class="n">acompletion</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
                <span class="c1">## LOGGING</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">post_call</span><span class="p">(</span>
                    <span class="nb">input</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
                    <span class="n">original_response</span><span class="o">=</span><span class="n">_response</span><span class="p">,</span>
                    <span class="n">additional_args</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;headers&quot;</span><span class="p">:</span> <span class="n">headers</span><span class="p">},</span>
                <span class="p">)</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">_response</span>

        <span class="k">elif</span> <span class="p">(</span>
            <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">open_ai_chat_completion_models</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;custom_openai&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;deepinfra&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;perplexity&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;groq&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;nvidia_nim&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;cerebras&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;ai21_chat&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;volcengine&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;codestral&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;deepseek&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;anyscale&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;mistral&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;openai&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;together_ai&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">openai_compatible_providers</span>
            <span class="ow">or</span> <span class="s2">&quot;ft:gpt-3.5-turbo&quot;</span> <span class="ow">in</span> <span class="n">model</span>  <span class="c1"># finetune gpt-3.5-turbo</span>
        <span class="p">):</span>  <span class="c1"># allow user to make an openai call with a custom base</span>
            <span class="c1"># note: if a user sets a custom base - we should ensure this works</span>
            <span class="c1"># allow for the setting of dynamic and stateful api-bases</span>
            <span class="n">api_base</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_base</span>  <span class="c1"># for deepinfra/perplexity/anyscale/groq/friendliai we check in get_llm_provider and pass in the api base from there</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;OPENAI_API_BASE&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="s2">&quot;https://api.openai.com/v1&quot;</span>
            <span class="p">)</span>
            <span class="n">openai</span><span class="o">.</span><span class="n">organization</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">organization</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">organization</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;OPENAI_ORGANIZATION&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="kc">None</span>  <span class="c1"># default - https://github.com/openai/openai-python/blob/284c1799070c723c6a553337134148a7ab088dd8/openai/util.py#L105</span>
            <span class="p">)</span>
            <span class="c1"># set API KEY</span>
            <span class="n">api_key</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_key</span>  <span class="c1"># for deepinfra/perplexity/anyscale/friendliai we check in get_llm_provider and pass in the api key from there</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">openai_key</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="n">headers</span> <span class="o">=</span> <span class="n">headers</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">headers</span>

            <span class="c1">## LOAD CONFIG - if set</span>
            <span class="n">config</span> <span class="o">=</span> <span class="n">litellm</span><span class="o">.</span><span class="n">OpenAIConfig</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">optional_params</span>
                <span class="p">):</span>  <span class="c1"># completion(top_k=3) &gt; openai_config(top_k=3) &lt;- allows for dynamic variables to be passed in</span>
                    <span class="n">optional_params</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>

            <span class="c1">## COMPLETION CALL</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">response</span> <span class="o">=</span> <span class="n">openai_chat_completions</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                    <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span>
                    <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                    <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                    <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
                    <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                    <span class="n">acompletion</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span>
                    <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                    <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                    <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                    <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                    <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                    <span class="n">custom_prompt_dict</span><span class="o">=</span><span class="n">custom_prompt_dict</span><span class="p">,</span>
                    <span class="n">client</span><span class="o">=</span><span class="n">client</span><span class="p">,</span>  <span class="c1"># pass AsyncOpenAI, OpenAI client</span>
                    <span class="n">organization</span><span class="o">=</span><span class="n">organization</span><span class="p">,</span>
                    <span class="n">custom_llm_provider</span><span class="o">=</span><span class="n">custom_llm_provider</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="c1">## LOGGING - log the original exception returned</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">post_call</span><span class="p">(</span>
                    <span class="nb">input</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
                    <span class="n">original_response</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">),</span>
                    <span class="n">additional_args</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;headers&quot;</span><span class="p">:</span> <span class="n">headers</span><span class="p">},</span>
                <span class="p">)</span>
                <span class="k">raise</span> <span class="n">e</span>

            <span class="k">if</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;stream&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                <span class="c1">## LOGGING</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">post_call</span><span class="p">(</span>
                    <span class="nb">input</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
                    <span class="n">original_response</span><span class="o">=</span><span class="n">response</span><span class="p">,</span>
                    <span class="n">additional_args</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;headers&quot;</span><span class="p">:</span> <span class="n">headers</span><span class="p">},</span>
                <span class="p">)</span>
        <span class="k">elif</span> <span class="p">(</span>
            <span class="s2">&quot;replicate&quot;</span> <span class="ow">in</span> <span class="n">model</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;replicate&quot;</span>
            <span class="ow">or</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">replicate_models</span>
        <span class="p">):</span>
            <span class="c1"># Setting the relevant API KEY for replicate, replicate defaults to using os.environ.get(&quot;REPLICATE_API_TOKEN&quot;)</span>
            <span class="n">replicate_key</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">replicate_key</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">replicate_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;REPLICATE_API_KEY&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;REPLICATE_API_TOKEN&quot;</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="n">api_base</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;REPLICATE_API_BASE&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="s2">&quot;https://api.replicate.com/v1&quot;</span>
            <span class="p">)</span>

            <span class="n">custom_prompt_dict</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">custom_prompt_dict</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">custom_prompt_dict</span>
            <span class="p">)</span>

            <span class="n">model_response</span> <span class="o">=</span> <span class="n">replicate</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>  <span class="c1"># for calculating input/output tokens</span>
                <span class="n">api_key</span><span class="o">=</span><span class="n">replicate_key</span><span class="p">,</span>
                <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="n">custom_prompt_dict</span><span class="o">=</span><span class="n">custom_prompt_dict</span><span class="p">,</span>
                <span class="n">acompletion</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;stream&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
                <span class="c1">## LOGGING</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">post_call</span><span class="p">(</span>
                    <span class="nb">input</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">api_key</span><span class="o">=</span><span class="n">replicate_key</span><span class="p">,</span>
                    <span class="n">original_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="n">response</span> <span class="o">=</span> <span class="n">model_response</span>
        <span class="k">elif</span> <span class="p">(</span>
            <span class="s2">&quot;clarifai&quot;</span> <span class="ow">in</span> <span class="n">model</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;clarifai&quot;</span>
            <span class="ow">or</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">clarifai_models</span>
        <span class="p">):</span>
            <span class="n">clarifai_key</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">clarifai_key</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">clarifai_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;CLARIFAI_API_KEY&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;CLARIFAI_API_TOKEN&quot;</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="n">api_base</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;CLARIFAI_API_BASE&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="s2">&quot;https://api.clarifai.com/v2&quot;</span>
            <span class="p">)</span>

            <span class="n">custom_prompt_dict</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">custom_prompt_dict</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">custom_prompt_dict</span>
            <span class="p">)</span>
            <span class="n">model_response</span> <span class="o">=</span> <span class="n">clarifai</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                <span class="n">acompletion</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span>
                <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>  <span class="c1"># for calculating input/output tokens</span>
                <span class="n">api_key</span><span class="o">=</span><span class="n">clarifai_key</span><span class="p">,</span>
                <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="n">custom_prompt_dict</span><span class="o">=</span><span class="n">custom_prompt_dict</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="p">(</span>
                <span class="s2">&quot;stream&quot;</span> <span class="ow">in</span> <span class="n">optional_params</span>
                <span class="ow">and</span> <span class="n">optional_params</span><span class="p">[</span><span class="s2">&quot;stream&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">True</span>
            <span class="p">):</span>
                <span class="c1"># don&#39;t try to access stream object,</span>
                <span class="c1">## LOGGING</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">post_call</span><span class="p">(</span>
                    <span class="nb">input</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
                    <span class="n">original_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;stream&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="ow">or</span> <span class="n">acompletion</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
                <span class="c1">## LOGGING</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">post_call</span><span class="p">(</span>
                    <span class="nb">input</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">api_key</span><span class="o">=</span><span class="n">clarifai_key</span><span class="p">,</span>
                    <span class="n">original_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">model_response</span>

        <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;anthropic&quot;</span><span class="p">:</span>
            <span class="n">api_key</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">anthropic_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;ANTHROPIC_API_KEY&quot;</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">custom_prompt_dict</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">custom_prompt_dict</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">custom_prompt_dict</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="p">(</span><span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;claude-2&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;claude-instant-1&quot;</span><span class="p">):</span>
                <span class="c1"># call anthropic /completion, only use this route for claude-2, claude-instant-1</span>
                <span class="n">api_base</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">api_base</span>
                    <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_base</span>
                    <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;ANTHROPIC_API_BASE&quot;</span><span class="p">)</span>
                    <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;ANTHROPIC_BASE_URL&quot;</span><span class="p">)</span>
                    <span class="ow">or</span> <span class="s2">&quot;https://api.anthropic.com/v1/complete&quot;</span>
                <span class="p">)</span>

                <span class="k">if</span> <span class="n">api_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">api_base</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span>
                    <span class="s2">&quot;/v1/complete&quot;</span>
                <span class="p">):</span>
                    <span class="n">api_base</span> <span class="o">+=</span> <span class="s2">&quot;/v1/complete&quot;</span>

                <span class="n">response</span> <span class="o">=</span> <span class="n">anthropic_text_completions</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                    <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                    <span class="n">acompletion</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span>
                    <span class="n">custom_prompt_dict</span><span class="o">=</span><span class="n">litellm</span><span class="o">.</span><span class="n">custom_prompt_dict</span><span class="p">,</span>
                    <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                    <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                    <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                    <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                    <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                    <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>  <span class="c1"># for calculating input/output tokens</span>
                    <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
                    <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                    <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># call /messages</span>
                <span class="c1"># default route for all anthropic models</span>
                <span class="n">api_base</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">api_base</span>
                    <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_base</span>
                    <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;ANTHROPIC_API_BASE&quot;</span><span class="p">)</span>
                    <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;ANTHROPIC_BASE_URL&quot;</span><span class="p">)</span>
                    <span class="ow">or</span> <span class="s2">&quot;https://api.anthropic.com/v1/messages&quot;</span>
                <span class="p">)</span>

                <span class="k">if</span> <span class="n">api_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">api_base</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span>
                    <span class="s2">&quot;/v1/messages&quot;</span>
                <span class="p">):</span>
                    <span class="n">api_base</span> <span class="o">+=</span> <span class="s2">&quot;/v1/messages&quot;</span>

                <span class="n">response</span> <span class="o">=</span> <span class="n">anthropic_chat_completions</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                    <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                    <span class="n">acompletion</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span>
                    <span class="n">custom_prompt_dict</span><span class="o">=</span><span class="n">litellm</span><span class="o">.</span><span class="n">custom_prompt_dict</span><span class="p">,</span>
                    <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                    <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                    <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                    <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                    <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                    <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>  <span class="c1"># for calculating input/output tokens</span>
                    <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
                    <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                    <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span>
                    <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>
                    <span class="n">client</span><span class="o">=</span><span class="n">client</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;stream&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="ow">or</span> <span class="n">acompletion</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
                <span class="c1">## LOGGING</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">post_call</span><span class="p">(</span>
                    <span class="nb">input</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
                    <span class="n">original_response</span><span class="o">=</span><span class="n">response</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">response</span>
        <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;nlp_cloud&quot;</span><span class="p">:</span>
            <span class="n">nlp_cloud_key</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">nlp_cloud_key</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;NLP_CLOUD_API_KEY&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_key</span>
            <span class="p">)</span>

            <span class="n">api_base</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;NLP_CLOUD_API_BASE&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="s2">&quot;https://api.nlpcloud.io/v1/gpu/&quot;</span>
            <span class="p">)</span>

            <span class="n">response</span> <span class="o">=</span> <span class="n">nlp_cloud</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
                <span class="n">api_key</span><span class="o">=</span><span class="n">nlp_cloud_key</span><span class="p">,</span>
                <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="p">(</span>
                <span class="s2">&quot;stream&quot;</span> <span class="ow">in</span> <span class="n">optional_params</span>
                <span class="ow">and</span> <span class="n">optional_params</span><span class="p">[</span><span class="s2">&quot;stream&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">True</span>
            <span class="p">):</span>
                <span class="c1"># don&#39;t try to access stream object,</span>
                <span class="n">response</span> <span class="o">=</span> <span class="n">CustomStreamWrapper</span><span class="p">(</span>
                    <span class="n">response</span><span class="p">,</span>
                    <span class="n">model</span><span class="p">,</span>
                    <span class="n">custom_llm_provider</span><span class="o">=</span><span class="s2">&quot;nlp_cloud&quot;</span><span class="p">,</span>
                    <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;stream&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="ow">or</span> <span class="n">acompletion</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
                <span class="c1">## LOGGING</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">post_call</span><span class="p">(</span>
                    <span class="nb">input</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
                    <span class="n">original_response</span><span class="o">=</span><span class="n">response</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="n">response</span> <span class="o">=</span> <span class="n">response</span>
        <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;aleph_alpha&quot;</span><span class="p">:</span>
            <span class="n">aleph_alpha_key</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">aleph_alpha_key</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;ALEPH_ALPHA_API_KEY&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;ALEPHALPHA_API_KEY&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_key</span>
            <span class="p">)</span>

            <span class="n">api_base</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;ALEPH_ALPHA_API_BASE&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="s2">&quot;https://api.aleph-alpha.com/complete&quot;</span>
            <span class="p">)</span>

            <span class="n">model_response</span> <span class="o">=</span> <span class="n">aleph_alpha</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
                <span class="n">default_max_tokens_to_sample</span><span class="o">=</span><span class="n">litellm</span><span class="o">.</span><span class="n">max_tokens</span><span class="p">,</span>
                <span class="n">api_key</span><span class="o">=</span><span class="n">aleph_alpha_key</span><span class="p">,</span>
                <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>  <span class="c1"># model call logging done inside the class as we make need to modify I/O to fit aleph alpha&#39;s requirements</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="p">(</span>
                <span class="s2">&quot;stream&quot;</span> <span class="ow">in</span> <span class="n">optional_params</span>
                <span class="ow">and</span> <span class="n">optional_params</span><span class="p">[</span><span class="s2">&quot;stream&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">True</span>
            <span class="p">):</span>
                <span class="c1"># don&#39;t try to access stream object,</span>
                <span class="n">response</span> <span class="o">=</span> <span class="n">CustomStreamWrapper</span><span class="p">(</span>
                    <span class="n">model_response</span><span class="p">,</span>
                    <span class="n">model</span><span class="p">,</span>
                    <span class="n">custom_llm_provider</span><span class="o">=</span><span class="s2">&quot;aleph_alpha&quot;</span><span class="p">,</span>
                    <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">return</span> <span class="n">response</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">model_response</span>
        <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;cohere&quot;</span><span class="p">:</span>
            <span class="n">cohere_key</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">cohere_key</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;COHERE_API_KEY&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;CO_API_KEY&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_key</span>
            <span class="p">)</span>

            <span class="n">api_base</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;COHERE_API_BASE&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="s2">&quot;https://api.cohere.ai/v1/generate&quot;</span>
            <span class="p">)</span>

            <span class="n">headers</span> <span class="o">=</span> <span class="n">headers</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">headers</span> <span class="ow">or</span> <span class="p">{}</span>
            <span class="k">if</span> <span class="n">headers</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">headers</span> <span class="o">=</span> <span class="p">{}</span>

            <span class="k">if</span> <span class="n">extra_headers</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">headers</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">extra_headers</span><span class="p">)</span>

            <span class="n">model_response</span> <span class="o">=</span> <span class="n">cohere_completion</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
                <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span>
                <span class="n">api_key</span><span class="o">=</span><span class="n">cohere_key</span><span class="p">,</span>
                <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>  <span class="c1"># model call logging done inside the class as we make need to modify I/O to fit aleph alpha&#39;s requirements</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="p">(</span>
                <span class="s2">&quot;stream&quot;</span> <span class="ow">in</span> <span class="n">optional_params</span>
                <span class="ow">and</span> <span class="n">optional_params</span><span class="p">[</span><span class="s2">&quot;stream&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">True</span>
            <span class="p">):</span>
                <span class="c1"># don&#39;t try to access stream object,</span>
                <span class="n">response</span> <span class="o">=</span> <span class="n">CustomStreamWrapper</span><span class="p">(</span>
                    <span class="n">model_response</span><span class="p">,</span>
                    <span class="n">model</span><span class="p">,</span>
                    <span class="n">custom_llm_provider</span><span class="o">=</span><span class="s2">&quot;cohere&quot;</span><span class="p">,</span>
                    <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">return</span> <span class="n">response</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">model_response</span>
        <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;cohere_chat&quot;</span><span class="p">:</span>
            <span class="n">cohere_key</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">cohere_key</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;COHERE_API_KEY&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;CO_API_KEY&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_key</span>
            <span class="p">)</span>

            <span class="n">api_base</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;COHERE_API_BASE&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="s2">&quot;https://api.cohere.ai/v1/chat&quot;</span>
            <span class="p">)</span>

            <span class="n">headers</span> <span class="o">=</span> <span class="n">headers</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">headers</span> <span class="ow">or</span> <span class="p">{}</span>
            <span class="k">if</span> <span class="n">headers</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">headers</span> <span class="o">=</span> <span class="p">{}</span>

            <span class="k">if</span> <span class="n">extra_headers</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">headers</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">extra_headers</span><span class="p">)</span>

            <span class="n">model_response</span> <span class="o">=</span> <span class="n">cohere_chat</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span>
                <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
                <span class="n">api_key</span><span class="o">=</span><span class="n">cohere_key</span><span class="p">,</span>
                <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>  <span class="c1"># model call logging done inside the class as we make need to modify I/O to fit aleph alpha&#39;s requirements</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="p">(</span>
                <span class="s2">&quot;stream&quot;</span> <span class="ow">in</span> <span class="n">optional_params</span>
                <span class="ow">and</span> <span class="n">optional_params</span><span class="p">[</span><span class="s2">&quot;stream&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">True</span>
            <span class="p">):</span>
                <span class="c1"># don&#39;t try to access stream object,</span>
                <span class="n">response</span> <span class="o">=</span> <span class="n">CustomStreamWrapper</span><span class="p">(</span>
                    <span class="n">model_response</span><span class="p">,</span>
                    <span class="n">model</span><span class="p">,</span>
                    <span class="n">custom_llm_provider</span><span class="o">=</span><span class="s2">&quot;cohere_chat&quot;</span><span class="p">,</span>
                    <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">return</span> <span class="n">response</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">model_response</span>
        <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;maritalk&quot;</span><span class="p">:</span>
            <span class="n">maritalk_key</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">maritalk_key</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;MARITALK_API_KEY&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_key</span>
            <span class="p">)</span>

            <span class="n">api_base</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;MARITALK_API_BASE&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="s2">&quot;https://chat.maritaca.ai/api/chat/inference&quot;</span>
            <span class="p">)</span>

            <span class="n">model_response</span> <span class="o">=</span> <span class="n">maritalk</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
                <span class="n">api_key</span><span class="o">=</span><span class="n">maritalk_key</span><span class="p">,</span>
                <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="p">(</span>
                <span class="s2">&quot;stream&quot;</span> <span class="ow">in</span> <span class="n">optional_params</span>
                <span class="ow">and</span> <span class="n">optional_params</span><span class="p">[</span><span class="s2">&quot;stream&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">True</span>
            <span class="p">):</span>
                <span class="c1"># don&#39;t try to access stream object,</span>
                <span class="n">response</span> <span class="o">=</span> <span class="n">CustomStreamWrapper</span><span class="p">(</span>
                    <span class="n">model_response</span><span class="p">,</span>
                    <span class="n">model</span><span class="p">,</span>
                    <span class="n">custom_llm_provider</span><span class="o">=</span><span class="s2">&quot;maritalk&quot;</span><span class="p">,</span>
                    <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">return</span> <span class="n">response</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">model_response</span>
        <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;huggingface&quot;</span><span class="p">:</span>
            <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;huggingface&quot;</span>
            <span class="n">huggingface_key</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">huggingface_key</span>
                <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;HF_TOKEN&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;HUGGINGFACE_API_KEY&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_key</span>
            <span class="p">)</span>
            <span class="n">hf_headers</span> <span class="o">=</span> <span class="n">headers</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">headers</span>

            <span class="n">custom_prompt_dict</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">custom_prompt_dict</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">custom_prompt_dict</span>
            <span class="p">)</span>
            <span class="n">model_response</span> <span class="o">=</span> <span class="n">huggingface</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">headers</span><span class="o">=</span><span class="n">hf_headers</span><span class="p">,</span>
                <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
                <span class="n">api_key</span><span class="o">=</span><span class="n">huggingface_key</span><span class="p">,</span>
                <span class="n">acompletion</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span>
                <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="n">custom_prompt_dict</span><span class="o">=</span><span class="n">custom_prompt_dict</span><span class="p">,</span>
                <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="s2">&quot;stream&quot;</span> <span class="ow">in</span> <span class="n">optional_params</span>
                <span class="ow">and</span> <span class="n">optional_params</span><span class="p">[</span><span class="s2">&quot;stream&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">True</span>
                <span class="ow">and</span> <span class="n">acompletion</span> <span class="ow">is</span> <span class="kc">False</span>
            <span class="p">):</span>
                <span class="c1"># don&#39;t try to access stream object,</span>
                <span class="n">response</span> <span class="o">=</span> <span class="n">CustomStreamWrapper</span><span class="p">(</span>
                    <span class="n">model_response</span><span class="p">,</span>
                    <span class="n">model</span><span class="p">,</span>
                    <span class="n">custom_llm_provider</span><span class="o">=</span><span class="s2">&quot;huggingface&quot;</span><span class="p">,</span>
                    <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">return</span> <span class="n">response</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">model_response</span>
        <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;oobabooga&quot;</span><span class="p">:</span>
            <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;oobabooga&quot;</span>
            <span class="n">model_response</span> <span class="o">=</span> <span class="n">oobabooga</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                <span class="n">api_key</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
                <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="s2">&quot;stream&quot;</span> <span class="ow">in</span> <span class="n">optional_params</span>
                <span class="ow">and</span> <span class="n">optional_params</span><span class="p">[</span><span class="s2">&quot;stream&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">True</span>
            <span class="p">):</span>
                <span class="c1"># don&#39;t try to access stream object,</span>
                <span class="n">response</span> <span class="o">=</span> <span class="n">CustomStreamWrapper</span><span class="p">(</span>
                    <span class="n">model_response</span><span class="p">,</span>
                    <span class="n">model</span><span class="p">,</span>
                    <span class="n">custom_llm_provider</span><span class="o">=</span><span class="s2">&quot;oobabooga&quot;</span><span class="p">,</span>
                    <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">return</span> <span class="n">response</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">model_response</span>
        <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;databricks&quot;</span><span class="p">:</span>
            <span class="n">api_base</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_base</span>  <span class="c1"># for databricks we check in get_llm_provider and pass in the api base from there</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;DATABRICKS_API_BASE&quot;</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="c1"># set API KEY</span>
            <span class="n">api_key</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_key</span>  <span class="c1"># for databricks we check in get_llm_provider and pass in the api key from there</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">databricks_key</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;DATABRICKS_API_KEY&quot;</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="n">headers</span> <span class="o">=</span> <span class="n">headers</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">headers</span>

            <span class="c1">## COMPLETION CALL</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">response</span> <span class="o">=</span> <span class="n">databricks_chat_completions</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                    <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span>
                    <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                    <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                    <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
                    <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                    <span class="n">acompletion</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span>
                    <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                    <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                    <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                    <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                    <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                    <span class="n">custom_prompt_dict</span><span class="o">=</span><span class="n">custom_prompt_dict</span><span class="p">,</span>
                    <span class="n">client</span><span class="o">=</span><span class="n">client</span><span class="p">,</span>  <span class="c1"># pass AsyncOpenAI, OpenAI client</span>
                    <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
                    <span class="n">custom_llm_provider</span><span class="o">=</span><span class="s2">&quot;databricks&quot;</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="c1">## LOGGING - log the original exception returned</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">post_call</span><span class="p">(</span>
                    <span class="nb">input</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
                    <span class="n">original_response</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">),</span>
                    <span class="n">additional_args</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;headers&quot;</span><span class="p">:</span> <span class="n">headers</span><span class="p">},</span>
                <span class="p">)</span>
                <span class="k">raise</span> <span class="n">e</span>

            <span class="k">if</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;stream&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                <span class="c1">## LOGGING</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">post_call</span><span class="p">(</span>
                    <span class="nb">input</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
                    <span class="n">original_response</span><span class="o">=</span><span class="n">response</span><span class="p">,</span>
                    <span class="n">additional_args</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;headers&quot;</span><span class="p">:</span> <span class="n">headers</span><span class="p">},</span>
                <span class="p">)</span>
        <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;openrouter&quot;</span><span class="p">:</span>
            <span class="n">api_base</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_base</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_base</span> <span class="ow">or</span> <span class="s2">&quot;https://openrouter.ai/api/v1&quot;</span>
            <span class="p">)</span>

            <span class="n">api_key</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">openrouter_key</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;OPENROUTER_API_KEY&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;OR_API_KEY&quot;</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="n">openrouter_site_url</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;OR_SITE_URL&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="s2">&quot;https://litellm.ai&quot;</span>
            <span class="p">)</span>
            <span class="n">openrouter_app_name</span> <span class="o">=</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;OR_APP_NAME&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="s2">&quot;liteLLM&quot;</span>

            <span class="n">openrouter_headers</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;HTTP-Referer&quot;</span><span class="p">:</span> <span class="n">openrouter_site_url</span><span class="p">,</span>
                <span class="s2">&quot;X-Title&quot;</span><span class="p">:</span> <span class="n">openrouter_app_name</span><span class="p">,</span>
            <span class="p">}</span>

            <span class="n">_headers</span> <span class="o">=</span> <span class="n">headers</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">headers</span>
            <span class="k">if</span> <span class="n">_headers</span><span class="p">:</span>
                <span class="n">openrouter_headers</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">_headers</span><span class="p">)</span>

            <span class="n">headers</span> <span class="o">=</span> <span class="n">openrouter_headers</span>

            <span class="c1">## Load Config</span>
            <span class="n">config</span> <span class="o">=</span> <span class="n">openrouter</span><span class="o">.</span><span class="n">OpenrouterConfig</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="s2">&quot;extra_body&quot;</span><span class="p">:</span>
                    <span class="c1"># we use openai &#39;extra_body&#39; to pass openrouter specific params - transforms, route, models</span>
                    <span class="k">if</span> <span class="s2">&quot;extra_body&quot;</span> <span class="ow">in</span> <span class="n">optional_params</span><span class="p">:</span>
                        <span class="n">optional_params</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">optional_params</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
                <span class="k">elif</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">optional_params</span><span class="p">:</span>
                    <span class="n">optional_params</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>

            <span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">model</span><span class="p">,</span> <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">messages</span><span class="p">,</span> <span class="o">**</span><span class="n">optional_params</span><span class="p">}</span>

            <span class="c1">## COMPLETION CALL</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">openai_chat_completions</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span>
                <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
                <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="n">acompletion</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span>
                <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">custom_llm_provider</span><span class="o">=</span><span class="s2">&quot;openrouter&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1">## LOGGING</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">post_call</span><span class="p">(</span>
                <span class="nb">input</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">api_key</span><span class="o">=</span><span class="n">openai</span><span class="o">.</span><span class="n">api_key</span><span class="p">,</span>
                <span class="n">original_response</span><span class="o">=</span><span class="n">response</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="p">(</span>
            <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;together_ai&quot;</span>
            <span class="ow">or</span> <span class="p">(</span><span class="s2">&quot;togethercomputer&quot;</span> <span class="ow">in</span> <span class="n">model</span><span class="p">)</span>
            <span class="ow">or</span> <span class="p">(</span><span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">together_ai_models</span><span class="p">)</span>
        <span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            Deprecated. We now do together ai calls via the openai client - https://docs.together.ai/docs/openai-api-compatibility</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="k">pass</span>
        <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;palm&quot;</span><span class="p">:</span>
            <span class="n">palm_api_key</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_key</span> <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;PALM_API_KEY&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_key</span>
            <span class="p">)</span>

            <span class="c1"># palm does not support streaming as yet :(</span>
            <span class="n">model_response</span> <span class="o">=</span> <span class="n">palm</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
                <span class="n">api_key</span><span class="o">=</span><span class="n">palm_api_key</span><span class="p">,</span>
                <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># fake palm streaming</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="s2">&quot;stream&quot;</span> <span class="ow">in</span> <span class="n">optional_params</span>
                <span class="ow">and</span> <span class="n">optional_params</span><span class="p">[</span><span class="s2">&quot;stream&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">True</span>
            <span class="p">):</span>
                <span class="c1"># fake streaming for palm</span>
                <span class="n">resp_string</span> <span class="o">=</span> <span class="n">model_response</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;message&quot;</span><span class="p">][</span>
                    <span class="s2">&quot;content&quot;</span>
                <span class="p">]</span>
                <span class="n">response</span> <span class="o">=</span> <span class="n">CustomStreamWrapper</span><span class="p">(</span>
                    <span class="n">resp_string</span><span class="p">,</span>
                    <span class="n">model</span><span class="p">,</span>
                    <span class="n">custom_llm_provider</span><span class="o">=</span><span class="s2">&quot;palm&quot;</span><span class="p">,</span>
                    <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">return</span> <span class="n">response</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">model_response</span>
        <span class="k">elif</span> <span class="p">(</span>
            <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;vertex_ai_beta&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;gemini&quot;</span>
        <span class="p">):</span>
            <span class="n">vertex_ai_project</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">optional_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;vertex_project&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;vertex_ai_project&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">vertex_project</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;VERTEXAI_PROJECT&quot;</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">vertex_ai_location</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">optional_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;vertex_location&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;vertex_ai_location&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">vertex_location</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;VERTEXAI_LOCATION&quot;</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">vertex_credentials</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">optional_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;vertex_credentials&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;vertex_ai_credentials&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;VERTEXAI_CREDENTIALS&quot;</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="n">gemini_api_key</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;GEMINI_API_KEY&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span>
                    <span class="s2">&quot;PALM_API_KEY&quot;</span>
                <span class="p">)</span>  <span class="c1"># older palm api key should also work</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_key</span>
            <span class="p">)</span>

            <span class="n">new_params</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">optional_params</span><span class="p">)</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">vertex_chat_completion</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                <span class="n">optional_params</span><span class="o">=</span><span class="n">new_params</span><span class="p">,</span>
                <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
                <span class="n">vertex_location</span><span class="o">=</span><span class="n">vertex_ai_location</span><span class="p">,</span>
                <span class="n">vertex_project</span><span class="o">=</span><span class="n">vertex_ai_project</span><span class="p">,</span>
                <span class="n">vertex_credentials</span><span class="o">=</span><span class="n">vertex_credentials</span><span class="p">,</span>
                <span class="n">gemini_api_key</span><span class="o">=</span><span class="n">gemini_api_key</span><span class="p">,</span>
                <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="n">acompletion</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span>
                <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>
                <span class="n">custom_llm_provider</span><span class="o">=</span><span class="n">custom_llm_provider</span><span class="p">,</span>
                <span class="n">client</span><span class="o">=</span><span class="n">client</span><span class="p">,</span>
                <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                <span class="n">extra_headers</span><span class="o">=</span><span class="n">extra_headers</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;vertex_ai&quot;</span><span class="p">:</span>
            <span class="n">vertex_ai_project</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">optional_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;vertex_project&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;vertex_ai_project&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">vertex_project</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;VERTEXAI_PROJECT&quot;</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">vertex_ai_location</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">optional_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;vertex_location&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;vertex_ai_location&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">vertex_location</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;VERTEXAI_LOCATION&quot;</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">vertex_credentials</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">optional_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;vertex_credentials&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;vertex_ai_credentials&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;VERTEXAI_CREDENTIALS&quot;</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="n">new_params</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">optional_params</span><span class="p">)</span>
            <span class="k">if</span> <span class="s2">&quot;claude-3&quot;</span> <span class="ow">in</span> <span class="n">model</span><span class="p">:</span>
                <span class="n">model_response</span> <span class="o">=</span> <span class="n">vertex_ai_anthropic</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                    <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                    <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                    <span class="n">optional_params</span><span class="o">=</span><span class="n">new_params</span><span class="p">,</span>
                    <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                    <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                    <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
                    <span class="n">vertex_location</span><span class="o">=</span><span class="n">vertex_ai_location</span><span class="p">,</span>
                    <span class="n">vertex_project</span><span class="o">=</span><span class="n">vertex_ai_project</span><span class="p">,</span>
                    <span class="n">vertex_credentials</span><span class="o">=</span><span class="n">vertex_credentials</span><span class="p">,</span>
                    <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                    <span class="n">acompletion</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span>
                    <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span>
                    <span class="n">custom_prompt_dict</span><span class="o">=</span><span class="n">custom_prompt_dict</span><span class="p">,</span>
                    <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>
                    <span class="n">client</span><span class="o">=</span><span class="n">client</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="p">(</span>
                <span class="n">model</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;meta/&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">model</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;mistral&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">model</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;codestral&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">model</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;jamba&quot;</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="n">model_response</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">vertex_partner_models_chat_completion</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                        <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                        <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                        <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                        <span class="n">optional_params</span><span class="o">=</span><span class="n">new_params</span><span class="p">,</span>
                        <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                        <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                        <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
                        <span class="n">vertex_location</span><span class="o">=</span><span class="n">vertex_ai_location</span><span class="p">,</span>
                        <span class="n">vertex_project</span><span class="o">=</span><span class="n">vertex_ai_project</span><span class="p">,</span>
                        <span class="n">vertex_credentials</span><span class="o">=</span><span class="n">vertex_credentials</span><span class="p">,</span>
                        <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                        <span class="n">acompletion</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span>
                        <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span>
                        <span class="n">custom_prompt_dict</span><span class="o">=</span><span class="n">custom_prompt_dict</span><span class="p">,</span>
                        <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>
                        <span class="n">client</span><span class="o">=</span><span class="n">client</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="s2">&quot;gemini&quot;</span> <span class="ow">in</span> <span class="n">model</span><span class="p">:</span>
                <span class="n">model_response</span> <span class="o">=</span> <span class="n">vertex_chat_completion</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
                    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                    <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                    <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                    <span class="n">optional_params</span><span class="o">=</span><span class="n">new_params</span><span class="p">,</span>
                    <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                    <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                    <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
                    <span class="n">vertex_location</span><span class="o">=</span><span class="n">vertex_ai_location</span><span class="p">,</span>
                    <span class="n">vertex_project</span><span class="o">=</span><span class="n">vertex_ai_project</span><span class="p">,</span>
                    <span class="n">vertex_credentials</span><span class="o">=</span><span class="n">vertex_credentials</span><span class="p">,</span>
                    <span class="n">gemini_api_key</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                    <span class="n">acompletion</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span>
                    <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>
                    <span class="n">custom_llm_provider</span><span class="o">=</span><span class="n">custom_llm_provider</span><span class="p">,</span>
                    <span class="n">client</span><span class="o">=</span><span class="n">client</span><span class="p">,</span>
                    <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                    <span class="n">extra_headers</span><span class="o">=</span><span class="n">extra_headers</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">model_response</span> <span class="o">=</span> <span class="n">vertex_ai_non_gemini</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                    <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                    <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                    <span class="n">optional_params</span><span class="o">=</span><span class="n">new_params</span><span class="p">,</span>
                    <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                    <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                    <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
                    <span class="n">vertex_location</span><span class="o">=</span><span class="n">vertex_ai_location</span><span class="p">,</span>
                    <span class="n">vertex_project</span><span class="o">=</span><span class="n">vertex_ai_project</span><span class="p">,</span>
                    <span class="n">vertex_credentials</span><span class="o">=</span><span class="n">vertex_credentials</span><span class="p">,</span>
                    <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                    <span class="n">acompletion</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span>
                <span class="p">)</span>

                <span class="k">if</span> <span class="p">(</span>
                    <span class="s2">&quot;stream&quot;</span> <span class="ow">in</span> <span class="n">optional_params</span>
                    <span class="ow">and</span> <span class="n">optional_params</span><span class="p">[</span><span class="s2">&quot;stream&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">True</span>
                    <span class="ow">and</span> <span class="n">acompletion</span> <span class="ow">is</span> <span class="kc">False</span>
                <span class="p">):</span>
                    <span class="n">response</span> <span class="o">=</span> <span class="n">CustomStreamWrapper</span><span class="p">(</span>
                        <span class="n">model_response</span><span class="p">,</span>
                        <span class="n">model</span><span class="p">,</span>
                        <span class="n">custom_llm_provider</span><span class="o">=</span><span class="s2">&quot;vertex_ai&quot;</span><span class="p">,</span>
                        <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="k">return</span> <span class="n">response</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">model_response</span>
        <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;predibase&quot;</span><span class="p">:</span>
            <span class="n">tenant_id</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">optional_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;tenant_id&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;predibase_tenant_id&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">predibase_tenant_id</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;PREDIBASE_TENANT_ID&quot;</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="n">api_base</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;api_base&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;base_url&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;PREDIBASE_API_BASE&quot;</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="n">api_key</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">predibase_key</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;PREDIBASE_API_KEY&quot;</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="n">_model_response</span> <span class="o">=</span> <span class="n">predibase_chat_completions</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
                <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="n">acompletion</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span>
                <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                <span class="n">custom_prompt_dict</span><span class="o">=</span><span class="n">custom_prompt_dict</span><span class="p">,</span>
                <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
                <span class="n">tenant_id</span><span class="o">=</span><span class="n">tenant_id</span><span class="p">,</span>
                <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="p">(</span>
                <span class="s2">&quot;stream&quot;</span> <span class="ow">in</span> <span class="n">optional_params</span>
                <span class="ow">and</span> <span class="n">optional_params</span><span class="p">[</span><span class="s2">&quot;stream&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">True</span>
                <span class="ow">and</span> <span class="n">acompletion</span> <span class="ow">is</span> <span class="kc">False</span>
            <span class="p">):</span>
                <span class="k">return</span> <span class="n">_model_response</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">_model_response</span>
        <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;text-completion-codestral&quot;</span><span class="p">:</span>
            <span class="n">api_base</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;api_base&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;base_url&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_base</span>
                <span class="ow">or</span> <span class="s2">&quot;https://codestral.mistral.ai/v1/fim/completions&quot;</span>
            <span class="p">)</span>

            <span class="n">api_key</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_key</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_key</span> <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;CODESTRAL_API_KEY&quot;</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="n">text_completion_model_response</span> <span class="o">=</span> <span class="n">litellm</span><span class="o">.</span><span class="n">TextCompletionResponse</span><span class="p">(</span>
                <span class="n">stream</span><span class="o">=</span><span class="n">stream</span>
            <span class="p">)</span>

            <span class="n">_model_response</span> <span class="o">=</span> <span class="n">codestral_text_completions</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">model_response</span><span class="o">=</span><span class="n">text_completion_model_response</span><span class="p">,</span>
                <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
                <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="n">acompletion</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span>
                <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                <span class="n">custom_prompt_dict</span><span class="o">=</span><span class="n">custom_prompt_dict</span><span class="p">,</span>
                <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
                <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="p">(</span>
                <span class="s2">&quot;stream&quot;</span> <span class="ow">in</span> <span class="n">optional_params</span>
                <span class="ow">and</span> <span class="n">optional_params</span><span class="p">[</span><span class="s2">&quot;stream&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">True</span>
                <span class="ow">and</span> <span class="n">acompletion</span> <span class="ow">is</span> <span class="kc">False</span>
            <span class="p">):</span>
                <span class="k">return</span> <span class="n">_model_response</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">_model_response</span>
        <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;ai21&quot;</span><span class="p">:</span>
            <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;ai21&quot;</span>
            <span class="n">ai21_key</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">ai21_key</span>
                <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;AI21_API_KEY&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_key</span>
            <span class="p">)</span>

            <span class="n">api_base</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;AI21_API_BASE&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="s2">&quot;https://api.ai21.com/studio/v1/&quot;</span>
            <span class="p">)</span>

            <span class="n">model_response</span> <span class="o">=</span> <span class="n">ai21</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
                <span class="n">api_key</span><span class="o">=</span><span class="n">ai21_key</span><span class="p">,</span>
                <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="p">(</span>
                <span class="s2">&quot;stream&quot;</span> <span class="ow">in</span> <span class="n">optional_params</span>
                <span class="ow">and</span> <span class="n">optional_params</span><span class="p">[</span><span class="s2">&quot;stream&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">True</span>
            <span class="p">):</span>
                <span class="c1"># don&#39;t try to access stream object,</span>
                <span class="n">response</span> <span class="o">=</span> <span class="n">CustomStreamWrapper</span><span class="p">(</span>
                    <span class="n">model_response</span><span class="p">,</span>
                    <span class="n">model</span><span class="p">,</span>
                    <span class="n">custom_llm_provider</span><span class="o">=</span><span class="s2">&quot;ai21&quot;</span><span class="p">,</span>
                    <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">return</span> <span class="n">response</span>

            <span class="c1">## RESPONSE OBJECT</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">model_response</span>
        <span class="k">elif</span> <span class="p">(</span>
            <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;sagemaker&quot;</span>
            <span class="ow">or</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;sagemaker_chat&quot;</span>
        <span class="p">):</span>
            <span class="c1"># boto3 reads keys from .env</span>
            <span class="n">model_response</span> <span class="o">=</span> <span class="n">sagemaker_llm</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                <span class="n">custom_prompt_dict</span><span class="o">=</span><span class="n">custom_prompt_dict</span><span class="p">,</span>
                <span class="n">hf_model_name</span><span class="o">=</span><span class="n">hf_model_name</span><span class="p">,</span>
                <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
                <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="n">acompletion</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span>
                <span class="n">use_messages_api</span><span class="o">=</span><span class="p">(</span>
                    <span class="kc">True</span> <span class="k">if</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;sagemaker_chat&quot;</span> <span class="k">else</span> <span class="kc">False</span>
                <span class="p">),</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;stream&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                <span class="c1">## LOGGING</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">post_call</span><span class="p">(</span>
                    <span class="nb">input</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">api_key</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">original_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="c1">## RESPONSE OBJECT</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">model_response</span>
        <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;bedrock&quot;</span><span class="p">:</span>
            <span class="c1"># boto3 reads keys from .env</span>
            <span class="n">custom_prompt_dict</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">custom_prompt_dict</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">custom_prompt_dict</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="s2">&quot;aws_bedrock_client&quot;</span> <span class="ow">in</span> <span class="n">optional_params</span><span class="p">:</span>
                <span class="n">verbose_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;&#39;aws_bedrock_client&#39; is a deprecated param. Please move to another auth method - https://docs.litellm.ai/docs/providers/bedrock#boto3---authentication.&quot;</span>
                <span class="p">)</span>
                <span class="c1"># Extract credentials for legacy boto3 client and pass thru to httpx</span>
                <span class="n">aws_bedrock_client</span> <span class="o">=</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;aws_bedrock_client&quot;</span><span class="p">)</span>
                <span class="n">creds</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">aws_bedrock_client</span><span class="o">.</span><span class="n">_get_credentials</span><span class="p">()</span><span class="o">.</span><span class="n">get_frozen_credentials</span><span class="p">()</span>
                <span class="p">)</span>

                <span class="k">if</span> <span class="n">creds</span><span class="o">.</span><span class="n">access_key</span><span class="p">:</span>
                    <span class="n">optional_params</span><span class="p">[</span><span class="s2">&quot;aws_access_key_id&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">creds</span><span class="o">.</span><span class="n">access_key</span>
                <span class="k">if</span> <span class="n">creds</span><span class="o">.</span><span class="n">secret_key</span><span class="p">:</span>
                    <span class="n">optional_params</span><span class="p">[</span><span class="s2">&quot;aws_secret_access_key&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">creds</span><span class="o">.</span><span class="n">secret_key</span>
                <span class="k">if</span> <span class="n">creds</span><span class="o">.</span><span class="n">token</span><span class="p">:</span>
                    <span class="n">optional_params</span><span class="p">[</span><span class="s2">&quot;aws_session_token&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">creds</span><span class="o">.</span><span class="n">token</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="s2">&quot;aws_region_name&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">optional_params</span>
                    <span class="ow">or</span> <span class="n">optional_params</span><span class="p">[</span><span class="s2">&quot;aws_region_name&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="p">):</span>
                    <span class="n">optional_params</span><span class="p">[</span>
                        <span class="s2">&quot;aws_region_name&quot;</span>
                    <span class="p">]</span> <span class="o">=</span> <span class="n">aws_bedrock_client</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">region_name</span>

            <span class="k">if</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">BEDROCK_CONVERSE_MODELS</span><span class="p">:</span>
                <span class="n">response</span> <span class="o">=</span> <span class="n">bedrock_converse_chat_completion</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                    <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">custom_prompt_dict</span><span class="o">=</span><span class="n">custom_prompt_dict</span><span class="p">,</span>
                    <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                    <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                    <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                    <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                    <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                    <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
                    <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                    <span class="n">extra_headers</span><span class="o">=</span><span class="n">extra_headers</span><span class="p">,</span>
                    <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>
                    <span class="n">acompletion</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span>
                    <span class="n">client</span><span class="o">=</span><span class="n">client</span><span class="p">,</span>
                    <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">response</span> <span class="o">=</span> <span class="n">bedrock_chat_completion</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                    <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">custom_prompt_dict</span><span class="o">=</span><span class="n">custom_prompt_dict</span><span class="p">,</span>
                    <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                    <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                    <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                    <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                    <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                    <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
                    <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                    <span class="n">extra_headers</span><span class="o">=</span><span class="n">extra_headers</span><span class="p">,</span>
                    <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>
                    <span class="n">acompletion</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span>
                    <span class="n">client</span><span class="o">=</span><span class="n">client</span><span class="p">,</span>
                    <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;stream&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                <span class="c1">## LOGGING</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">post_call</span><span class="p">(</span>
                    <span class="nb">input</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">api_key</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">original_response</span><span class="o">=</span><span class="n">response</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="c1">## RESPONSE OBJECT</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">response</span>
        <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;watsonx&quot;</span><span class="p">:</span>
            <span class="n">custom_prompt_dict</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">custom_prompt_dict</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">custom_prompt_dict</span>
            <span class="p">)</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">watsonxai</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">custom_prompt_dict</span><span class="o">=</span><span class="n">custom_prompt_dict</span><span class="p">,</span>
                <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
                <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">acompletion</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="s2">&quot;stream&quot;</span> <span class="ow">in</span> <span class="n">optional_params</span>
                <span class="ow">and</span> <span class="n">optional_params</span><span class="p">[</span><span class="s2">&quot;stream&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">True</span>
                <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">CustomStreamWrapper</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="c1"># don&#39;t try to access stream object,</span>
                <span class="n">response</span> <span class="o">=</span> <span class="n">CustomStreamWrapper</span><span class="p">(</span>
                    <span class="nb">iter</span><span class="p">(</span><span class="n">response</span><span class="p">),</span>
                    <span class="n">model</span><span class="p">,</span>
                    <span class="n">custom_llm_provider</span><span class="o">=</span><span class="s2">&quot;watsonx&quot;</span><span class="p">,</span>
                    <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;stream&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                <span class="c1">## LOGGING</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">post_call</span><span class="p">(</span>
                    <span class="nb">input</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">api_key</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">original_response</span><span class="o">=</span><span class="n">response</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="c1">## RESPONSE OBJECT</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">response</span>
        <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;vllm&quot;</span><span class="p">:</span>
            <span class="n">custom_prompt_dict</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">custom_prompt_dict</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">custom_prompt_dict</span>
            <span class="p">)</span>
            <span class="n">model_response</span> <span class="o">=</span> <span class="n">vllm</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">custom_prompt_dict</span><span class="o">=</span><span class="n">custom_prompt_dict</span><span class="p">,</span>
                <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
                <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="p">(</span>
                <span class="s2">&quot;stream&quot;</span> <span class="ow">in</span> <span class="n">optional_params</span>
                <span class="ow">and</span> <span class="n">optional_params</span><span class="p">[</span><span class="s2">&quot;stream&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">True</span>
            <span class="p">):</span>  <span class="c1">## [BETA]</span>
                <span class="c1"># don&#39;t try to access stream object,</span>
                <span class="n">response</span> <span class="o">=</span> <span class="n">CustomStreamWrapper</span><span class="p">(</span>
                    <span class="n">model_response</span><span class="p">,</span>
                    <span class="n">model</span><span class="p">,</span>
                    <span class="n">custom_llm_provider</span><span class="o">=</span><span class="s2">&quot;vllm&quot;</span><span class="p">,</span>
                    <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">return</span> <span class="n">response</span>

            <span class="c1">## RESPONSE OBJECT</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">model_response</span>
        <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;ollama&quot;</span><span class="p">:</span>
            <span class="n">api_base</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">litellm</span><span class="o">.</span><span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;OLLAMA_API_BASE&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="s2">&quot;http://localhost:11434&quot;</span>
            <span class="p">)</span>
            <span class="n">custom_prompt_dict</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">custom_prompt_dict</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">custom_prompt_dict</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">custom_prompt_dict</span><span class="p">:</span>
                <span class="c1"># check if the model has a registered custom prompt</span>
                <span class="n">model_prompt_details</span> <span class="o">=</span> <span class="n">custom_prompt_dict</span><span class="p">[</span><span class="n">model</span><span class="p">]</span>
                <span class="n">prompt</span> <span class="o">=</span> <span class="n">custom_prompt</span><span class="p">(</span>
                    <span class="n">role_dict</span><span class="o">=</span><span class="n">model_prompt_details</span><span class="p">[</span><span class="s2">&quot;roles&quot;</span><span class="p">],</span>
                    <span class="n">initial_prompt_value</span><span class="o">=</span><span class="n">model_prompt_details</span><span class="p">[</span>
                        <span class="s2">&quot;initial_prompt_value&quot;</span>
                    <span class="p">],</span>
                    <span class="n">final_prompt_value</span><span class="o">=</span><span class="n">model_prompt_details</span><span class="p">[</span>
                        <span class="s2">&quot;final_prompt_value&quot;</span>
                    <span class="p">],</span>
                    <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">prompt</span> <span class="o">=</span> <span class="n">prompt_factory</span><span class="p">(</span>
                    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                    <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">custom_llm_provider</span><span class="o">=</span><span class="n">custom_llm_provider</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                    <span class="c1"># for multimode models - ollama/llava prompt_factory returns a dict {</span>
                    <span class="c1">#     &quot;prompt&quot;: prompt,</span>
                    <span class="c1">#     &quot;images&quot;: images</span>
                    <span class="c1"># }</span>
                    <span class="n">prompt</span><span class="p">,</span> <span class="n">images</span> <span class="o">=</span> <span class="n">prompt</span><span class="p">[</span><span class="s2">&quot;prompt&quot;</span><span class="p">],</span> <span class="n">prompt</span><span class="p">[</span><span class="s2">&quot;images&quot;</span><span class="p">]</span>
                    <span class="n">optional_params</span><span class="p">[</span><span class="s2">&quot;images&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">images</span>

            <span class="c1">## LOGGING</span>
            <span class="n">generator</span> <span class="o">=</span> <span class="n">ollama</span><span class="o">.</span><span class="n">get_ollama_response</span><span class="p">(</span>
                <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
                <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="n">acompletion</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span>
                <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">acompletion</span> <span class="ow">is</span> <span class="kc">True</span>
                <span class="ow">or</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;stream&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="o">==</span> <span class="kc">True</span>
            <span class="p">):</span>
                <span class="k">return</span> <span class="n">generator</span>

            <span class="n">response</span> <span class="o">=</span> <span class="n">generator</span>
        <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;ollama_chat&quot;</span><span class="p">:</span>
            <span class="n">api_base</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">litellm</span><span class="o">.</span><span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;OLLAMA_API_BASE&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="s2">&quot;http://localhost:11434&quot;</span>
            <span class="p">)</span>

            <span class="n">api_key</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">ollama_key</span>
                <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;OLLAMA_API_KEY&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_key</span>
            <span class="p">)</span>
            <span class="c1">## LOGGING</span>
            <span class="n">generator</span> <span class="o">=</span> <span class="n">ollama_chat</span><span class="o">.</span><span class="n">get_ollama_response</span><span class="p">(</span>
                <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="n">acompletion</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span>
                <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">acompletion</span> <span class="ow">is</span> <span class="kc">True</span>
                <span class="ow">or</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;stream&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">True</span>
            <span class="p">):</span>
                <span class="k">return</span> <span class="n">generator</span>

            <span class="n">response</span> <span class="o">=</span> <span class="n">generator</span>

        <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;triton&quot;</span><span class="p">:</span>
            <span class="n">api_base</span> <span class="o">=</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_base</span> <span class="ow">or</span> <span class="n">api_base</span>
            <span class="n">model_response</span> <span class="o">=</span> <span class="n">triton_chat_completions</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="n">stream</span><span class="o">=</span><span class="n">stream</span><span class="p">,</span>
                <span class="n">acompletion</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1">## RESPONSE OBJECT</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">model_response</span>
            <span class="k">return</span> <span class="n">response</span>

        <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;cloudflare&quot;</span><span class="p">:</span>
            <span class="n">api_key</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">cloudflare_api_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;CLOUDFLARE_API_KEY&quot;</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">account_id</span> <span class="o">=</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;CLOUDFLARE_ACCOUNT_ID&quot;</span><span class="p">)</span>
            <span class="n">api_base</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_base</span>
                <span class="ow">or</span> <span class="n">get_secret</span><span class="p">(</span><span class="s2">&quot;CLOUDFLARE_API_BASE&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="sa">f</span><span class="s2">&quot;https://api.cloudflare.com/client/v4/accounts/</span><span class="si">{</span><span class="n">account_id</span><span class="si">}</span><span class="s2">/ai/run/&quot;</span>
            <span class="p">)</span>

            <span class="n">custom_prompt_dict</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">custom_prompt_dict</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">custom_prompt_dict</span>
            <span class="p">)</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">cloudflare</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                <span class="n">custom_prompt_dict</span><span class="o">=</span><span class="n">litellm</span><span class="o">.</span><span class="n">custom_prompt_dict</span><span class="p">,</span>
                <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>  <span class="c1"># for calculating input/output tokens</span>
                <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
                <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="s2">&quot;stream&quot;</span> <span class="ow">in</span> <span class="n">optional_params</span>
                <span class="ow">and</span> <span class="n">optional_params</span><span class="p">[</span><span class="s2">&quot;stream&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">True</span>
            <span class="p">):</span>
                <span class="c1"># don&#39;t try to access stream object,</span>
                <span class="n">response</span> <span class="o">=</span> <span class="n">CustomStreamWrapper</span><span class="p">(</span>
                    <span class="n">response</span><span class="p">,</span>
                    <span class="n">model</span><span class="p">,</span>
                    <span class="n">custom_llm_provider</span><span class="o">=</span><span class="s2">&quot;cloudflare&quot;</span><span class="p">,</span>
                    <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;stream&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="ow">or</span> <span class="n">acompletion</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
                <span class="c1">## LOGGING</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">post_call</span><span class="p">(</span>
                    <span class="nb">input</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
                    <span class="n">original_response</span><span class="o">=</span><span class="n">response</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">response</span>
        <span class="k">elif</span> <span class="p">(</span>
            <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;baseten&quot;</span>
            <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_base</span> <span class="o">==</span> <span class="s2">&quot;https://app.baseten.co&quot;</span>
        <span class="p">):</span>
            <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;baseten&quot;</span>
            <span class="n">baseten_key</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">api_key</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">baseten_key</span>
                <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;BASETEN_API_KEY&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_key</span>
            <span class="p">)</span>

            <span class="n">model_response</span> <span class="o">=</span> <span class="n">baseten</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
                <span class="n">api_key</span><span class="o">=</span><span class="n">baseten_key</span><span class="p">,</span>
                <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">inspect</span><span class="o">.</span><span class="n">isgenerator</span><span class="p">(</span><span class="n">model_response</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
                <span class="s2">&quot;stream&quot;</span> <span class="ow">in</span> <span class="n">optional_params</span>
                <span class="ow">and</span> <span class="n">optional_params</span><span class="p">[</span><span class="s2">&quot;stream&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">True</span>
            <span class="p">):</span>
                <span class="c1"># don&#39;t try to access stream object,</span>
                <span class="n">response</span> <span class="o">=</span> <span class="n">CustomStreamWrapper</span><span class="p">(</span>
                    <span class="n">model_response</span><span class="p">,</span>
                    <span class="n">model</span><span class="p">,</span>
                    <span class="n">custom_llm_provider</span><span class="o">=</span><span class="s2">&quot;baseten&quot;</span><span class="p">,</span>
                    <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">return</span> <span class="n">response</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">model_response</span>
        <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;petals&quot;</span> <span class="ow">or</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">petals_models</span><span class="p">:</span>
            <span class="n">api_base</span> <span class="o">=</span> <span class="n">api_base</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_base</span>

            <span class="n">custom_llm_provider</span> <span class="o">=</span> <span class="s2">&quot;petals&quot;</span>
            <span class="n">stream</span> <span class="o">=</span> <span class="n">optional_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;stream&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="n">model_response</span> <span class="o">=</span> <span class="n">petals</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
                <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">stream</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>  <span class="c1">## [BETA]</span>
                <span class="c1"># Fake streaming for petals</span>
                <span class="n">resp_string</span> <span class="o">=</span> <span class="n">model_response</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;message&quot;</span><span class="p">][</span>
                    <span class="s2">&quot;content&quot;</span>
                <span class="p">]</span>
                <span class="n">response</span> <span class="o">=</span> <span class="n">CustomStreamWrapper</span><span class="p">(</span>
                    <span class="n">resp_string</span><span class="p">,</span>
                    <span class="n">model</span><span class="p">,</span>
                    <span class="n">custom_llm_provider</span><span class="o">=</span><span class="s2">&quot;petals&quot;</span><span class="p">,</span>
                    <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">return</span> <span class="n">response</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">model_response</span>
        <span class="k">elif</span> <span class="n">custom_llm_provider</span> <span class="o">==</span> <span class="s2">&quot;custom&quot;</span><span class="p">:</span>
            <span class="kn">import</span> <span class="nn">requests</span>

            <span class="n">url</span> <span class="o">=</span> <span class="n">litellm</span><span class="o">.</span><span class="n">api_base</span> <span class="ow">or</span> <span class="n">api_base</span> <span class="ow">or</span> <span class="s2">&quot;&quot;</span>
            <span class="k">if</span> <span class="n">url</span> <span class="o">==</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">url</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;api_base not set. Set api_base or litellm.api_base for custom endpoints&quot;</span>
                <span class="p">)</span>

<span class="w">            </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            assume input to custom LLM api bases follow this format:</span>
<span class="sd">            resp = requests.post(</span>
<span class="sd">                api_base,</span>
<span class="sd">                json={</span>
<span class="sd">                    &#39;model&#39;: &#39;meta-llama/Llama-2-13b-hf&#39;, # model name</span>
<span class="sd">                    &#39;params&#39;: {</span>
<span class="sd">                        &#39;prompt&#39;: [&quot;The capital of France is P&quot;],</span>
<span class="sd">                        &#39;max_tokens&#39;: 32,</span>
<span class="sd">                        &#39;temperature&#39;: 0.7,</span>
<span class="sd">                        &#39;top_p&#39;: 1.0,</span>
<span class="sd">                        &#39;top_k&#39;: 40,</span>
<span class="sd">                    }</span>
<span class="sd">                }</span>
<span class="sd">            )</span>

<span class="sd">            &quot;&quot;&quot;</span>
            <span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">message</span><span class="p">[</span><span class="s2">&quot;content&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">])</span>  <span class="c1"># type: ignore</span>
            <span class="n">resp</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
                <span class="n">url</span><span class="p">,</span>
                <span class="n">json</span><span class="o">=</span><span class="p">{</span>
                    <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">model</span><span class="p">,</span>
                    <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">prompt</span><span class="p">],</span>
                        <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="n">max_tokens</span><span class="p">,</span>
                        <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="n">temperature</span><span class="p">,</span>
                        <span class="s2">&quot;top_p&quot;</span><span class="p">:</span> <span class="n">top_p</span><span class="p">,</span>
                        <span class="s2">&quot;top_k&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;top_k&quot;</span><span class="p">,</span> <span class="mi">40</span><span class="p">),</span>
                    <span class="p">},</span>
                <span class="p">},</span>
                <span class="n">verify</span><span class="o">=</span><span class="n">litellm</span><span class="o">.</span><span class="n">ssl_verify</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">response_json</span> <span class="o">=</span> <span class="n">resp</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            assume all responses from custom api_bases of this format:</span>
<span class="sd">            {</span>
<span class="sd">                &#39;data&#39;: [</span>
<span class="sd">                    {</span>
<span class="sd">                        &#39;prompt&#39;: &#39;The capital of France is P&#39;,</span>
<span class="sd">                        &#39;output&#39;: [&#39;The capital of France is PARIS.\nThe capital of France is PARIS.\nThe capital of France is PARIS.\nThe capital of France is PARIS.\nThe capital of France is PARIS.\nThe capital of France is PARIS.\nThe capital of France is PARIS.\nThe capital of France is PARIS.\nThe capital of France is PARIS.\nThe capital of France is PARIS.\nThe capital of France is PARIS.\nThe capital of France is PARIS.\nThe capital of France is PARIS.\nThe capital of France&#39;],</span>
<span class="sd">                        &#39;params&#39;: {&#39;temperature&#39;: 0.7, &#39;top_k&#39;: 40, &#39;top_p&#39;: 1}}],</span>
<span class="sd">                        &#39;message&#39;: &#39;ok&#39;</span>
<span class="sd">                    }</span>
<span class="sd">                ]</span>
<span class="sd">            }</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="n">string_response</span> <span class="o">=</span> <span class="n">response_json</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;output&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="c1">## RESPONSE OBJECT</span>
            <span class="n">model_response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span> <span class="o">=</span> <span class="n">string_response</span>  <span class="c1"># type: ignore</span>
            <span class="n">model_response</span><span class="o">.</span><span class="n">created</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">())</span>
            <span class="n">model_response</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">model_response</span>
        <span class="k">elif</span> <span class="p">(</span>
            <span class="n">custom_llm_provider</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">_custom_providers</span>
        <span class="p">):</span>  <span class="c1"># Assume custom LLM provider</span>
            <span class="c1"># Get the Custom Handler</span>
            <span class="n">custom_handler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CustomLLM</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">litellm</span><span class="o">.</span><span class="n">custom_provider_map</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">item</span><span class="p">[</span><span class="s2">&quot;provider&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">custom_llm_provider</span><span class="p">:</span>
                    <span class="n">custom_handler</span> <span class="o">=</span> <span class="n">item</span><span class="p">[</span><span class="s2">&quot;custom_handler&quot;</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">custom_handler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Unable to map your input to a model. Check your input - </span><span class="si">{</span><span class="n">args</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

            <span class="c1">## ROUTE LLM CALL ##</span>
            <span class="n">handler_fn</span> <span class="o">=</span> <span class="n">custom_chat_llm_router</span><span class="p">(</span>
                <span class="n">async_fn</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="n">stream</span><span class="p">,</span> <span class="n">custom_llm</span><span class="o">=</span><span class="n">custom_handler</span>
            <span class="p">)</span>

            <span class="n">headers</span> <span class="o">=</span> <span class="n">headers</span> <span class="ow">or</span> <span class="n">litellm</span><span class="o">.</span><span class="n">headers</span>

            <span class="c1">## CALL FUNCTION</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">handler_fn</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span>
                <span class="n">model_response</span><span class="o">=</span><span class="n">model_response</span><span class="p">,</span>
                <span class="n">print_verbose</span><span class="o">=</span><span class="n">print_verbose</span><span class="p">,</span>
                <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
                <span class="n">api_base</span><span class="o">=</span><span class="n">api_base</span><span class="p">,</span>
                <span class="n">acompletion</span><span class="o">=</span><span class="n">acompletion</span><span class="p">,</span>
                <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="n">optional_params</span><span class="o">=</span><span class="n">optional_params</span><span class="p">,</span>
                <span class="n">litellm_params</span><span class="o">=</span><span class="n">litellm_params</span><span class="p">,</span>
                <span class="n">logger_fn</span><span class="o">=</span><span class="n">logger_fn</span><span class="p">,</span>
                <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">custom_prompt_dict</span><span class="o">=</span><span class="n">custom_prompt_dict</span><span class="p">,</span>
                <span class="n">client</span><span class="o">=</span><span class="n">client</span><span class="p">,</span>  <span class="c1"># pass AsyncOpenAI, OpenAI client</span>
                <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">stream</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">CustomStreamWrapper</span><span class="p">(</span>
                    <span class="n">completion_stream</span><span class="o">=</span><span class="n">response</span><span class="p">,</span>
                    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                    <span class="n">custom_llm_provider</span><span class="o">=</span><span class="n">custom_llm_provider</span><span class="p">,</span>
                    <span class="n">logging_obj</span><span class="o">=</span><span class="n">logging</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Unable to map your input to a model. Check your input - </span><span class="si">{</span><span class="n">args</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">response</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="c1">## Map to OpenAI Exception</span>
        <span class="k">raise</span> <span class="n">exception_type</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">custom_llm_provider</span><span class="o">=</span><span class="n">custom_llm_provider</span><span class="p">,</span>
            <span class="n">original_exception</span><span class="o">=</span><span class="n">e</span><span class="p">,</span>
            <span class="n">completion_kwargs</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
            <span class="n">extra_kwargs</span><span class="o">=</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span></div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, NotDiamond.
      <span class="lastupdated">Last updated on Jan 28, 2025.
      </span></p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>